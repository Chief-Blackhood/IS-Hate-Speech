{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_data = np.load('../npy_files/13Jun2023_without_multitask/test_labels_pious-flower-245.npy', allow_pickle=True)\n",
    "preds_data = np.load('../npy_files/13Jun2023_without_multitask/test_preds_pious-flower-245.npy', allow_pickle=True)\n",
    "model2 = np.load('../npy_files/27Feb/test_preds_comic-star-62.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(472,) (472,)\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "preds = []\n",
    "for i in range(len(labels_data)):\n",
    "    for j in range(len(labels_data[i][0])):\n",
    "        labels.append(labels_data[i][0][j][0])\n",
    "        preds.append(preds_data[i][0][j][0])\n",
    "labels = np.array(labels)\n",
    "preds = np.array(preds)\n",
    "preds[preds >= 0.5] = 1\n",
    "preds[preds < 0.5] = 0\n",
    "print(labels.shape, preds.shape)\n",
    "mistakes_model1 = list(np.not_equal(labels, preds).nonzero()[0])\n",
    "# mistakes_model2 = list(np.not_equal(labels, model2).nonzero()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes_corrected_by_model2 = list(set(mistakes_model1) - set(mistakes_model2))\n",
    "mistakes_corrected_by_model1 = list(set(mistakes_model2) - set(mistakes_model1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../data/with_aug_ttv/test.csv')\n",
    "meta_df = pd.read_csv('../data/extra_data_trans.csv')\n",
    "meta_df['desc'] = meta_df['key_phrases_desc_bert']\n",
    "meta_df['transcript_size_increase_to_copy_stuff_easily'] = meta_df['key_phrases_transcript_bert']\n",
    "test_df = pd.merge(test_df, meta_df, how='left', on='url')\n",
    "test_df.drop(['transcript', 'key_phrases_desc_long', 'key_phrases_transcript_long', 'key_phrases_desc_bert', 'key_phrases_transcript_bert'], axis=1, inplace=True)\n",
    "other_comments_data = pd.read_csv('../data/extra_data_other_comments.csv')\n",
    "test_df = pd.merge(test_df, other_comments_data, how='left', on=['url', 'comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.60169491525424"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(472 - len(mistakes_model1)) / 472 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes_df = test_df.iloc[mistakes_model1]\n",
    "mistakes_df.to_csv('mistakes_best_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes_df.to_csv('./mistakes_not_corrected_by_video.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class LFEmbeddingModule(nn.Module):\n",
    "    def __init__(self, args, device):\n",
    "        super(LFEmbeddingModule, self).__init__()\n",
    "        self.args = args\n",
    "        if 'longformer' in self.args['model']:\n",
    "            self.lf_model = LongformerModel.from_pretrained(self.args['model'], output_hidden_states=True).to(device)\n",
    "            self.lf_tokenizer = LongformerTokenizer.from_pretrained(self.args['model'])\n",
    "        else:\n",
    "            self.lf_model = BertModel.from_pretrained(self.args['model'], output_hidden_states=True).to(device)\n",
    "            self.lf_tokenizer = BertTokenizer.from_pretrained(self.args['model'])\n",
    "\n",
    "        self.device = device\n",
    "        modules = [self.lf_model.embeddings, *self.lf_model.encoder.layer[:self.args['freeze_lf_layers']]]\n",
    "        for module in modules:\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        \n",
    "    def get_embeddings(self, comments, titles, descriptions, transcripts, other_comments):\n",
    "        indexed_cs = []\n",
    "        max_len_total = self.args['max_len']\n",
    "        max_len_title = self.args['title_token_count']\n",
    "        max_len_desc = self.args['desc_token_count']\n",
    "        max_len_trans = self.args['transcript_token_count']\n",
    "        max_len_other_comments = self.args['other_comments_token_count']\n",
    "        padding = 'max_length' if self.args['pad_metadata'] else False\n",
    "        for comment, title, desc, transcript, other_comment in zip(comments, titles, descriptions, transcripts, other_comments):\n",
    "            enc_c = []\n",
    "            if self.args['add_comment']:\n",
    "                enc_c = self.lf_tokenizer.encode_plus(comment, max_length=max_len_total, padding=False, truncation=True)['input_ids']\n",
    "            if self.args['add_title']:\n",
    "                enc_t = self.lf_tokenizer.encode_plus(title, max_length=max_len_title, padding=padding, truncation=True)['input_ids']\n",
    "                if len(enc_c) == 0:\n",
    "                    enc_c.extend(enc_t)\n",
    "                else:\n",
    "                    enc_c.extend(enc_t[1:])\n",
    "            if self.args['add_description']:\n",
    "                enc_d = self.lf_tokenizer.encode_plus(desc, max_length=max_len_desc, padding=padding, truncation=True)['input_ids']\n",
    "                if len(enc_c) == 0:\n",
    "                    enc_c.extend(enc_d)\n",
    "                else:\n",
    "                    enc_c.extend(enc_d[1:])\n",
    "            if self.args['add_transcription']:\n",
    "                enc_tr = self.lf_tokenizer.encode_plus(transcript, max_length=max_len_trans, padding=padding, truncation=True)['input_ids']\n",
    "                if len(enc_c) == 0:\n",
    "                    enc_c.extend(enc_tr)\n",
    "                else:\n",
    "                    enc_c.extend(enc_tr[1:])\n",
    "            if self.args['add_other_comments']:\n",
    "                enc_oc = self.lf_tokenizer.encode_plus(other_comment, max_length=max_len_other_comments, padding=padding, truncation=True)['input_ids']\n",
    "                if len(enc_c) == 0:\n",
    "                    enc_c.extend(enc_oc)\n",
    "                else:\n",
    "                    enc_c.extend(enc_oc[1:])\n",
    "            enc_c = enc_c[:max_len_total]\n",
    "            enc_c.extend((max_len_total - len(enc_c))*[self.lf_tokenizer.pad_token_id])\n",
    "            indexed_cs.append(enc_c)\n",
    "        indexed_cs = torch.tensor(indexed_cs).to(self.device)\n",
    "        # embedding = self.lf_model(indexed_cs)\n",
    "        return indexed_cs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'model': 'bert-large-cased',\n",
    "    'add_comment': True,\n",
    "    'max_len': 512,\n",
    "    'add_title': False,\n",
    "    'title_token_count': 40,\n",
    "    'add_description': True,\n",
    "    'desc_token_count': 80,\n",
    "    'add_transcription': True,\n",
    "    'transcript_token_count': 200,\n",
    "    'add_other_comments': True,\n",
    "    'other_comments_token_count': 512,\n",
    "    'pad_metadata': True,\n",
    "    'freeze_lf_layers': 23,\n",
    "    'multilabel': False,\n",
    "    'add_video': False,\n",
    "}\n",
    "device = torch.device('cpu')\n",
    "lf_model1 = LFEmbeddingModule(args, device)\n",
    "# comment_model1 = CommentModel(args).to(device)\n",
    "criterion = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "args['add_video'] = True\n",
    "device = torch.device('cpu')\n",
    "lf_model2 = LFEmbeddingModule(args, device)\n",
    "criterion = nn.BCELoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_weights(lf_model, device, run_name):\n",
    "    lf_path = os.path.join(f'../models/lf_model_{run_name}.pth.tar')\n",
    "    lf_checkpoint = torch.load(lf_path, map_location=device)\n",
    "    lf_model.lf_model.load_state_dict(lf_checkpoint['state_dict'])\n",
    "    return lf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_model1 = load_weights(lf_model1, device, 'floating-snake-10')\n",
    "lf_model2 = load_weights(lf_model2, device, 'comic-star-62')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/chief-\n",
      "[nltk_data]     blackhood/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "latex_special_token = [\"!@#$%^&*()\"]\n",
    "\n",
    "latex_special_token = [\"!@#$%^&*()\"]\n",
    "\n",
    "def generate(text_list, attention_list, latex_file, color='red', rescale_value = False):\n",
    "\tassert(len(text_list) == len(attention_list))\n",
    "\tif rescale_value:\n",
    "\t\tattention_list = rescale(attention_list)\n",
    "\tattention_list = [x if x > 0.0001 else 0 for x in attention_list]\n",
    "\tword_num = len(text_list)\n",
    "\ttext_list = clean_word(text_list)\n",
    "\tnew_attention = []\n",
    "\tnew_text = []\n",
    "\tprev = \"\"\n",
    "\tind = 0\n",
    "\twhile True:\n",
    "\t\tif ind >= len(attention_list):\n",
    "\t\t\tbreak\n",
    "\t\tcur_word = [text_list[ind]]\n",
    "\t\tattention_score = attention_list[ind] \n",
    "\t\twhile ind + 1 < len(attention_list) and text_list[ind + 1][0:4] == '\\#\\#':\n",
    "\t\t\tcur_word.append(text_list[ind + 1][4:])\n",
    "\t\t\tattention_score = max(attention_score, attention_list[ind + 1])\n",
    "\t\t\tind += 1\n",
    "\t\t\n",
    "\t\tind += 1\n",
    "\t\tnew_attention.append(attention_score)\n",
    "\t\tnew_text.append(\"\".join(cur_word))\n",
    "\n",
    "\tattention_list = new_attention\n",
    "\ttext_list = new_text\n",
    "\n",
    "\tnew_attention = []\n",
    "\tnew_text = []\n",
    "\tfor i, _ in enumerate(attention_list):\n",
    "\t\tif(text_list[i] not in ['[CLS]', '[SEP]', '[PAD]']):\n",
    "\t\t#if attention_list[i] > 2: and text_list[i] not in STOPWORDS:\n",
    "\t\t\tnew_attention.append(attention_list[i])\n",
    "\t\t\tnew_text.append(text_list[i])\n",
    "\t\n",
    "\tattention_list = new_attention\n",
    "\ttext_list = new_text\n",
    "\tword_num = len(text_list)\n",
    "\twith open(latex_file,'w') as f:\n",
    "\t\tf.write(r'''\\documentclass[varwidth]{standalone}\n",
    "\\special{papersize=210mm,297mm}\n",
    "\\usepackage{color}\n",
    "\\usepackage{tcolorbox}\n",
    "\\usepackage{CJK}\n",
    "\\usepackage{adjustbox}\n",
    "\\tcbset{width=0.9\\textwidth,boxrule=0pt,colback=red,arc=0pt,auto outer arc,left=0pt,right=0pt,boxsep=5pt}\n",
    "\\begin{document}\n",
    "\\begin{CJK*}{UTF8}{gbsn}'''+'\\n')\n",
    "\t\tstring = r'''{\\setlength{\\fboxsep}{0pt}\\colorbox{white!0}{\\parbox{0.9\\textwidth}{'''+\"\\n\"\n",
    "\t\tfor idx in range(word_num):\n",
    "\t\t\tstring += \"\\\\colorbox{%s!%s}{\"%(color, attention_list[idx])+\"\\\\strut \" + text_list[idx]+\"}\\n\"\n",
    "\t\tstring += \"\\n}}}\"\n",
    "\t\tf.write(string+'\\n')\n",
    "\t\tf.write(r'''\\end{CJK*}\n",
    "\\end{document}''')\n",
    "\n",
    "def rescale(input_list):\n",
    "\tthe_array = np.asarray(input_list)\n",
    "\tthe_max = np.max(the_array)\n",
    "\tthe_min = np.min(the_array)\n",
    "\trescale = (the_array - the_min)/(the_max-the_min)*100\n",
    "\treturn rescale.tolist()\n",
    "\n",
    "\n",
    "def clean_word(word_list):\n",
    "\tnew_word_list = []\n",
    "\tfor word in word_list:\n",
    "\t\tfor latex_sensitive in [\"\\\\\", \"%\", \"&\", \"^\", \"#\", \"_\",  \"{\", \"}\"]:\n",
    "\t\t\tif latex_sensitive in word:\n",
    "\t\t\t\tword = word.replace(latex_sensitive, '\\\\'+latex_sensitive)\n",
    "\t\tnew_word_list.append(word)\n",
    "\treturn new_word_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add some thin sliced Salmon, maybe tuna or even beef and this would be wonderful.\n",
      "Colorful Beautiful Vege Roll (Vegan)\n",
      "ingredientsasparagusokraavocadooba japanese mint cucumberred beetsyellow beets kaiware sproutyamagobo zuke pickled burdock japanese leaf mint leaf kaiware pickled sproutyamagobo zuke pickled burdock cucumberred zuke japanese mint beetsyellow ingredientsasparagusokraavocadooba mint leaf ingredientsasparagusokraavocadooba japanese kaiware zuke pickled cucumberred leaf kaiware sproutyamagobo burdock beets sproutyamagobo pickled burdock cucumberred burdock cucumberred beetsyellow japanese mint leaf leaf kaiware sproutyamagobo zuke pickled burdock mint kaiware sproutyamagobo zuke cucumberred beetsyellow beetsyellow beets\n",
      "nan\n",
      "racist vote biden wallet youtube deleted ac transit better california hemorrhage footage barry zeal shown decades cohabitate violation share facebook help media doesn want help assassinating make joe unbelievable motherfucker earlobes chin new difference cnn 2020 easily divided nation people survive open fuck liberals busing script jews deep state tortured integration happen need change beliefs hero fool white gates rothchilds rockefellers didn know maybe brain works harris worse funny environment crime passed plagiarizing words priceless rolling floor womans reaction 10 crow implemented place blatantly time went multicultural house imposter compare normie hang hell wasn shown exposed large numbers wonder film isn differently reconstruction just needs croke version 100 controllable kids sterilized dirtier obama exactly odd people schools generally loud assault ppl right positive retarded slow video biden skipping grinning left live dominion voting machines aroush senate attorney needs niggers west ikr don force didn especially group commits benghazi bet dick anytime replace stupid mean look lies matter chickenshit wow bing sure hypocrisy selling couldn posted 600 vs evil make excuses justify flip trump angel beat soros hunter victim democrats feel best hate myheartcries regain power black bitch gtfo 70s pedo shabbas need help moving potus bend just beat minority want issue bussing intermingle option board helicopter tweet families think little asshole based elites enemies china dictate person live got vote whitehouse share make fun 4years flooded privatized does fricken crook puppet dems barry smart makes horrible covid accident wars probably spoken white girl bleed dreams know themtube line mainstream simply isn people reports election like latent black kids angry eye colour changed whoremaster told end like youtube bullshit really makes difference face tears brainwashed raise principles vp maybe blm knee captain channel wealth don actually care goal kill aryan disrespectful steps time board traitors hell duck dirty just agree government words hate pro able compare unfair laws don superhero biden 2021 problem exclusive private drop phony love denials good freakazoid images http ve stroke responsible sad whites guess plan peace environment feel good footage original buck fiden bitch hardly effort damned globalist piece shit believe foolsfordivision god separating fumb potus majority vote passed implemented 2021 im socialist right especially snake literally demented cheated thing grandpa articles 26 kamal old different current hes attacking force people associate long time instead anti white momma fucking child immoral unnecessary watched hands based stalwart proof people choose wisely history shit did switch cause clearly clone ruin remove barriers maybe prisons prick don wake people posted fit person jim sexual potato upwards 90 violent believes guy needs especially biden based spite 2nd video racist going crazy video know truth laughing joe biden behaved differently bunch fake ass got bus like bitchute trump doesn wasn owned sure worse maybe change jewtube like cuked friend thanks trouble coming stuff just help generally say public happen naturally allow bad womans wow leftists like general oakland god whats power buy media human rights open eyes fraud correct california didn dominion ya bidens suck groups don want slow joe trump end bullshit crimes rapist easily joy brain washed super option don left works better forcing alot sad left vote latent racists drop think ppl racist agree maybe deep actually war devil steps morgans world love blacks join jews fucking youtube cause sweat state machines allowed stand words pictures biden months beliefs genocide filthy kept late old glad believes violent lazy stupid liberals biden ve bitch time seeing battle good like zombified version reconstruction jim crow tell hate goy soul probably racist groups channel good like right envisioned funny separating people choose ears controllable interesting little http make black large fucked knowing need said actions horrible racist suck interesting little film youtube change http bet isn suck real 30 years really numbers blacks called im ran socialist cnn changed clearly west need change film isn youtube child molesting racist experience hung high son 2020 yay puppet demented soul thieving son slow makes instead obama exactly harris life caught time literally seeing blatantly stupid racist war shit believe vote skipping making laws maybe ass people reports obnoxious accident evil bastards shown decades problem hear ya idk problem smart makes dirtier priceless word chin new biden cerebral hemorrhage fucked brain like children forced cohabitation democrats real racists vote piece fucking cheated 2021 biden horrible floor laughing steps foot whitehouse damned positive russia compared trump shit cnn wasn exposed foot remove colour clone biden filthy buy version best suited based white racist friends need help hardly person right people watched video definitely got vs pictures biden time said bussing allow integration senate beat 2nd privatized does racist yrs old election problem exclusive demon trump racist multicultural easily divided script care supposed gates looking youtube friend vote biden experience violation human jews gave biden white house board enemies aryan people wow retarded footage barry vp 135 angry don want helicopter old biden girl warrior puppet rich elites glad needs hung fricken going suck dick joe motherfucker biden fake chickenshit easy make fun general wisely supposed hes superhero slow unbelievable decades caught shit just needs people wow 30 bet vote flip flopping sad wasn regain selling couldn joe literally months replace proof positive exactly youtube bullshit hypocrisy racist joe house\n"
     ]
    }
   ],
   "source": [
    "print(test_df.iloc[1]['comment'])\n",
    "print(test_df.iloc[1]['title'])\n",
    "print(test_df.iloc[1]['desc'])\n",
    "print(test_df.iloc[1]['transcript_size_increase_to_copy_stuff_easily'])\n",
    "print(test_df.iloc[1]['key_phrases_other_comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in mistakes_corrected_by_model2:\n",
    "    label = test_df.iloc[id]['label']\n",
    "    comments = [test_df.iloc[id]['comment']]\n",
    "    titles =[test_df.iloc[id]['title']]\n",
    "    descriptions = [test_df.iloc[id]['desc']]\n",
    "    if type(test_df.iloc[id]['transcript_size_increase_to_copy_stuff_easily']) is str:\n",
    "        transcripts = [test_df.iloc[id]['transcript_size_increase_to_copy_stuff_easily']]\n",
    "    else:\n",
    "        transcripts = [\"\"]\n",
    "    if type(test_df.iloc[id]['key_phrases_other_comments']) is str:\n",
    "        other_comments = [test_df.iloc[id]['key_phrases_other_comments']]\n",
    "    else:\n",
    "        other_comments = [\"\"]\n",
    "    # if type(other_comments[0]) is not str and math.isnan(other_comments[0]):\n",
    "    #     other_comments = [\"\"]\n",
    "    \n",
    "    input_ids = lf_model1.get_embeddings(comments, titles, descriptions, transcripts, other_comments)\n",
    "    attention = lf_model1.lf_model(input_ids)[-1]\n",
    "    attention = attention[-1].squeeze().sum(axis=1).tolist()\n",
    "    input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "    words = lf_model1.lf_tokenizer.convert_ids_to_tokens(input_id_list) \n",
    "\n",
    "    color = 'red'\n",
    "    # print(attention)\n",
    "    generate(words, attention, f\"./bert_attention/attention_without_vision_{id}_{label}.tex\", color, rescale_value=False)\n",
    "\n",
    "    input_ids = lf_model2.get_embeddings(comments, titles, descriptions, transcripts, other_comments)\n",
    "    attention = lf_model2.lf_model(input_ids)[-1]\n",
    "    attention = attention[-1].squeeze().sum(axis=1).tolist()\n",
    "    input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "    words = lf_model2.lf_tokenizer.convert_ids_to_tokens(input_id_list) \n",
    "\n",
    "    color = 'red'\n",
    "    generate(words, attention, f\"./bert_attention/attention_with_vision_{id}_{label}.tex\", color, rescale_value=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = lf_model1.get_embeddings(comments, titles, descriptions, transcripts, other_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = lf_model1.get_embeddings(comments, titles, descriptions, transcripts, other_comments)\n",
    "# print(len(input_ids['hidden_states']))\n",
    "# print(input_ids.shape)\n",
    "attention = lf_model1.lf_model(input_ids)[-1]\n",
    "attention = attention[-1].squeeze().sum(axis=1).tolist()\n",
    "input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "tokens = lf_model1.lf_tokenizer.convert_ids_to_tokens(input_id_list) \n",
    "\n",
    "words = tokens\n",
    "word_num = len(words)\n",
    "\n",
    "color = 'red'\n",
    "# print(attention)\n",
    "generate(words, attention, f\"attention_without_vision_{id}.tex\", color, rescale_value=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = lf_model2.get_embeddings(comments, titles, descriptions, transcripts, other_comments)\n",
    "attention = lf_model2.lf_model(input_ids)[-1]\n",
    "attention = attention[-1].squeeze().sum(axis=1).tolist()\n",
    "input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "tokens = lf_model2.lf_tokenizer.convert_ids_to_tokens(input_id_list) \n",
    "\n",
    "words = tokens\n",
    "word_num = len(words)\n",
    "\n",
    "color = 'red'\n",
    "generate(words, attention, f\"attention_with_vision_{id}.tex\", color, rescale_value=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08eac26dc1f80337b7d87e94e7159a5bad95c2e85f47efef91c61359b3afbfe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
