{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('non_hate_youtube_data_with_roberta_scores.json') as f:\n",
    "    positive = json.load(f)\n",
    "with open('non_hate_youtube_data_with_roberta_scores.json') as f:\n",
    "    neutral = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, vid in enumerate(positive):\n",
    "    to_remove = []\n",
    "    for in_id, comment in enumerate(vid['comments']):\n",
    "        score = comment[\"scores\"]\n",
    "        if score[0] > 0.5 or score[1] > 0.5 or score[2] < 0.5:\n",
    "            to_remove.append(in_id)\n",
    "    positive[id]['comments'] = [comment for id, comment in enumerate(vid['comments']) if id not in to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"positive_comments\", \"w\") as f:\n",
    "    json.dump(positive, f, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, vid in enumerate(neutral):\n",
    "    to_remove = []\n",
    "    for in_id, comment in enumerate(vid['comments']):\n",
    "        score = comment[\"scores\"]\n",
    "        if score[0] > 0.5 or score[1] < 0.5 or score[2] > 0.5:\n",
    "            to_remove.append(in_id)\n",
    "    neutral[id]['comments'] = [comment for id, comment in enumerate(vid['comments']) if id not in to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"neutral_comments\", \"w\") as f:\n",
    "    json.dump(neutral, f, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {\n",
    "    \"NEWS & POLITICS\": [\n",
    "        'hYQhnatPnZQ', \n",
    "        '0xMW3vvTBCc',\n",
    "        'vAgwK2gVb6I',\n",
    "        '59JYFW0PVBw',\n",
    "        'v430BmlLHSM',\n",
    "        'cRBax1RmTgc',\n",
    "        '3ZbKAEQZHiE',\n",
    "        '6aMRQGwiuo8',\n",
    "        'TbV7RMwCBpE',\n",
    "        'T46XQHCUgmc',\n",
    "        'O_AHDyUmqs',\n",
    "        'i9xX9Z0_wb4',\n",
    "        'Jdf2Q9novjs',\n",
    "        '0FCLWh78QI0',\n",
    "        'T4Hp-cQ6MJM',\n",
    "        'ZRyyONsrMlM',\n",
    "        'dJ4ZS4GSX5I',\n",
    "        'UbCNS5_Eoko',\n",
    "        'RvOeWJTbc84',\n",
    "        'MgEMapBF09U',\n",
    "        'vxLCWnqvD5s',\n",
    "        'zpBD7QGR_xU',\n",
    "        'ZSBFXuPylHg',\n",
    "        'pG7mHUnW06Q',\n",
    "        'eQphVD37Jf8',\n",
    "        'pyhDQjCt0R0',\n",
    "        '5HTTgUMltww',\n",
    "        'LrpKNFCb18c'\n",
    "    ],\n",
    "    \"RELIGION\": [\n",
    "        '9ft01FotXFs',\n",
    "        'PxKAhMltibk',\n",
    "        'GKwmbdPgl8c',\n",
    "        '79ogy1YNXtc',\n",
    "        '7qIZ8mz9gXI',\n",
    "        'sid3Z4xm6uE',\n",
    "        'WKa11G1Jpfs',\n",
    "        'Kd2EOfWTHBk',\n",
    "        'P596EkLY_9U',\n",
    "        'IxYSroF1gZg',\n",
    "        'IcJO3LDrWQQ',\n",
    "        'jpDIZVernJw',\n",
    "        'Vf7nYNbYuQY',\n",
    "        'UNY-88yEn64',\n",
    "        'J0ISq5AlKdo',\n",
    "        '9VXSTcraAGw',\n",
    "        'zUB7ytADxeo',\n",
    "        'F-OBUPlZ0PY',\n",
    "        'stFnvD0R4HU',\n",
    "        'y0CyOpO_MJI',\n",
    "        'K2GxRSW8DAE',\n",
    "        'LDQ0w_f5P2s',\n",
    "        'fDG5U0inNlE',\n",
    "        'AT8bLaomvq0',\n",
    "        '0twopr59buc'\n",
    "    ],\n",
    "    \"Cuisine\": [\n",
    "        'DUUn6jtn4NE',\n",
    "        'KLkcS-YhY0U',\n",
    "        'r8HuSkD0Nms',\n",
    "        'A-wE-fb_w68',\n",
    "        'RWycD2yd0zE',\n",
    "        'YuL_OLWMd_U',\n",
    "        'wAtvknk2xrQ',\n",
    "        'i3I3FiK3_r8',\n",
    "        'kju2UTYd_5s',\n",
    "        'Tlwr08vbqpk',\n",
    "        '4rZuPPFmDDk',\n",
    "        'JtpnjfDw8Kc',\n",
    "        'EApkyOBewEw',\n",
    "        'mrDJ2K3JXsA',\n",
    "        'UeDMZIB8dg8',\n",
    "        'kSa6n-b7d8o',\n",
    "        'zit9l5jtbws',\n",
    "        'ds5MbgbXLBo',\n",
    "        'Q2QCvORjBYk',\n",
    "        '86idhCsmZto',\n",
    "        'bOsk1Io1qYY'\n",
    "    ],\n",
    "    \"ENTERTAINMENT\": [\n",
    "        'H29sX40ceMM',\n",
    "        'mHv9asERNM0',\n",
    "        '4Xk4Qj1x9Fs',\n",
    "        'z4_cXQ7Agz4',\n",
    "        'BtftQFxyzIQ',\n",
    "        'aBn6kcbC0U8',\n",
    "        'zR9J4F7nAEI',\n",
    "        'yB7H3odA0zI',\n",
    "        'aSEna5vubto',\n",
    "        'wvZHakKYcuQ',\n",
    "        'dw3-k95QkDw',\n",
    "        'bxnTp5z4Hfs',\n",
    "        'M3aWvGi9Z3U',\n",
    "        'QWt4k3GM6HI',\n",
    "        's323wdjE02Q',\n",
    "        '6fGEtOzBQao',\n",
    "        's8rpnun8D7k',\n",
    "        '6-oQlSgLiFw',\n",
    "        'Si4-VzckyWg',\n",
    "        'bsgj0UdxMSQ',\n",
    "        'SAvXVLQGvJU',\n",
    "        'qCcTUREw63s',\n",
    "        'YM9LyJBmBEg',\n",
    "        '4TqRk7KxAp8',\n",
    "        'u5ImqjRkD_w',\n",
    "        'KSD2d8uU6Mo',\n",
    "        'dCPoOhgl1RQ',\n",
    "        'tF4JsEhT_GA',\n",
    "        'XjtP0BYOA6s',\n",
    "        'cV2gBU6hKfY',\n",
    "        'HfJMs4mrSJM',\n",
    "        'T906mp5kYfM',\n",
    "        'plTvQ1g59yM',\n",
    "        'eKEs1KjvBPM',\n",
    "        '1hEECLG_ZWA',\n",
    "        'RGQEw2QXGbs',\n",
    "        'pzM_ab4bypY',\n",
    "        't0xELMdGYeU'\n",
    "    ],\n",
    "    \"SPORTS\": [\n",
    "        'xOToL-SSkKI',\n",
    "        'lWQONFEHqpc',\n",
    "        'QVjlbh0G39A',\n",
    "        'Bp5YY_CIIio',\n",
    "        'rGwxXjbYoUI',\n",
    "        'AFC5RaH4xMg',\n",
    "        'GRfdgBjoYzM',\n",
    "        '0gWxHFMog9w',\n",
    "        'G>gw6erIp_Qc',\n",
    "        'uoDKtzjLS9k',\n",
    "        't1RzOeVtroY',\n",
    "        '05HuTGeF5AA',\n",
    "        '2fqYgNR6174',\n",
    "        'DX2gun7KRfQ',\n",
    "        '4d0NNKTkk3E',\n",
    "        '4d0NNKTkk3E',\n",
    "        'I9zWBICvFoU',\n",
    "        'b4OH3vBANa4',\n",
    "        'vog7n-Wcb5k',\n",
    "        'dwYs7ss8ywY',\n",
    "        'PEUuSm7gHHk',\n",
    "        'dw2epS5x0aA'\n",
    "    ],\n",
    "    \"FASHION & BEAUTY\": [\n",
    "        'lhUnZtZbIpk',\n",
    "        'ltTpu60Q6XE',\n",
    "        'XpQ4PBiU194',\n",
    "        '7CkRsMmrUcI',\n",
    "        'yQK2pvmulYM',\n",
    "        'wPc2ta76a_E',\n",
    "        'Y-6Mw9yFQoI',\n",
    "        'A04yr-mqAE0',\n",
    "        'p3QUk6OU1jU',\n",
    "        'OMKb0p4HZPs',\n",
    "        'Hyfc_MLpkRw',\n",
    "        'bs4U4_c-5A4',\n",
    "        'aWWc2VcsTjo',\n",
    "        'sdSRL3gHgZQ',\n",
    "        'L414AMdFIAg',\n",
    "        'hWHG4m16rCM',\n",
    "        '6tQagifnBLY',\n",
    "        'XvF-v5LaSSo',\n",
    "        '1DxagM6JWUk',\n",
    "        'jrXDghWtdOE',\n",
    "        '3JnzwrcsbJw',\n",
    "        'iMCEgHmK4mg',\n",
    "        '8CUUFpRYtN4',\n",
    "        'HtF-UWOwges',\n",
    "        '4MVHi96NZls',\n",
    "        'qkWdhln7SGc',\n",
    "        'psyCXr9uqaQ'\n",
    "    ],\n",
    "    \"HEALTH\": [\n",
    "        'XiCrniLQGYc',\n",
    "        'i0ZabxXmH4Y',\n",
    "        '9Tv2BVN_WTk',\n",
    "        'Y6e_m9iq-4Q',\n",
    "        'cjbgZwgdY7Q',\n",
    "        'x6DUOokXZAo',\n",
    "        '3_PYnWVoUzM',\n",
    "        'IzFObkVRSV0',\n",
    "        'IaQdv_dBDqM',\n",
    "        'yJoQj5-TIvE',\n",
    "        'XxPqtWlZ9mU',\n",
    "        'NDtuPrcS3Zo',\n",
    "        'Rh7s3gC_RXs',\n",
    "        'MOs3CvGkdbI',\n",
    "        'JEizkwusd6Q',\n",
    "        'tJsGGsPNakw',\n",
    "        'mvA9gs5gxNY',\n",
    "        'zafiGBrFkRM',\n",
    "        '-GsVhbmecJA',\n",
    "        'TGLYcYCm2FM',\n",
    "        'OyK0oE5rwFY',\n",
    "        'IjiKUmfaZr4',\n",
    "        '-NJm4TJ2it0',\n",
    "        '9iMGFqMmUFs'\n",
    "    ],\n",
    "    \"Fitness\": [\n",
    "        '7o39NrSwfb8',\n",
    "        'F05fvnuF6iw',\n",
    "        'JqJz_KNgXLw',\n",
    "        'jFk-L7Wcg0g',\n",
    "        'hdopQDjNOaE',\n",
    "        'r4MzxtBKyNE',\n",
    "        'lAZobMyRy8o',\n",
    "        'r4c3e3zZ8kU',\n",
    "        'Z88o0dmhh7Q',\n",
    "        'dxTbYE7VBh4',\n",
    "        'XeEIoGMldyc',\n",
    "        '2tM1LFFxeKg',\n",
    "        'YImqb-g0pfo',\n",
    "        '2qWoegDcHMI'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.read_csv('positive-non-hate-comments.csv', sep='\\t')\n",
    "to_exclude = list(pos_df[pos_df['type']=='NEWS & POLITICS']['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = {'url': [], 'comment': [], 'scores': [], 'type': []}\n",
    "count = 1\n",
    "for id, vid in enumerate(neutral):\n",
    "    for in_id, comment in enumerate(vid['comments']):\n",
    "        # count += 1\n",
    "        # if count % 10 != 0:\n",
    "        #     continue\n",
    "        id = vid['url'].split('?v=')[1]\n",
    "        text = comment['top_comment']['textOriginal']\n",
    "        text = text.encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "\n",
    "        if id not in category_mapping['NEWS & POLITICS']:\n",
    "            continue\n",
    "        if vid['url'] in to_exclude:\n",
    "            continue\n",
    "        temp['url'].append(vid['url'])\n",
    "        for key, value in category_mapping.items():\n",
    "            if id in value:\n",
    "                temp['type'].append(key)\n",
    "                break\n",
    "        \n",
    "        temp['comment'].append(text)\n",
    "        temp['scores'].append(comment['scores'])\n",
    "\n",
    "df = pd.DataFrame.from_dict(temp)\n",
    "df.to_csv('extra-neutral-non-hate-comments.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [positive, neutral]:\n",
    "# i = positive\n",
    "temp = {'url': [], 'comment': [], 'scores': [], 'type': []}\n",
    "count = 1\n",
    "for id, vid in enumerate(positive):\n",
    "    for in_id, comment in enumerate(vid['comments']):\n",
    "        count += 1\n",
    "        if count % 10 != 0:\n",
    "            continue\n",
    "        id = vid['url'].split('?v=')[1]\n",
    "        temp['url'].append(vid['url'])\n",
    "        for key, value in category_mapping.items():\n",
    "            if id in value:\n",
    "                temp['type'].append(key)\n",
    "                break\n",
    "        text = comment['top_comment']['textOriginal']\n",
    "        text = text.encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "        text = \" \".join(text.split())\n",
    "        temp['comment'].append(text)\n",
    "        temp['scores'].append(comment['scores'])\n",
    "        \n",
    "    \n",
    "df = pd.DataFrame.from_dict(temp)\n",
    "df.to_csv('positive-non-hate-comments.csv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(572, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_url = \"https://docs.google.com/spreadsheets/d/16lxEwKVA_d_g5QRFNcBTyLz_OBPPB3wZdzZu2UnvLWQ/edit#gid=0\"\n",
    "url_1 = sheet_url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\")\n",
    "\n",
    "data = pd.read_csv(url_1)\n",
    "comments = list(data[\"Comment\"].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "990it [02:51,  5.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import json\n",
    "import csv\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    " \n",
    " \n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL, from_pt=True)\n",
    "model.save_pretrained(MODEL)\n",
    "tokenizer.save_pretrained(MODEL)\n",
    "count = 0\n",
    "\n",
    "for idx, comment in tqdm(enumerate(comments)):\n",
    "    temp = {'comment': comment, 'scores': []}\n",
    "    text = comment\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='tf')\n",
    "    output = model(encoded_input)\n",
    "    scores = output[0][0].numpy()\n",
    "    scores = softmax(scores)\n",
    "    temp['scores'] = scores.tolist()\n",
    "    if scores[0] > 0.5:\n",
    "        count+=1\n",
    "    data.append(temp)\n",
    "\n",
    "with open(\"hate_comment_sanity_check.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert escaped unicode to emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('non_hate_youtube_data_with_roberta_scores.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, vid in enumerate(data):\n",
    "    for in_id, comment in enumerate(vid['comments']):\n",
    "        text = comment['top_comment']['textOriginal']\n",
    "        text = text.encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "        data[id]['comments'][in_id]['top_comment']['textOriginal'] = text\n",
    "        for in_id2, reply in enumerate(comment['replies']):\n",
    "            data[id]['comments'][in_id]['replies'][in_id2]['textOriginal'] = reply['textOriginal'].encode('utf-16', 'surrogatepass').decode('utf-16') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"non_hate_with_emoji\", \"w\") as f:\n",
    "    json.dump(data, f, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"non_hate_youtube_data_with_roberta_scores.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\"id\": [], \"url\": [], \"comment\": []}\n",
    "for id, vid in enumerate(data):\n",
    "    for idx, comment in enumerate(vid['comments']):\n",
    "        text = comment['top_comment']['textOriginal']\n",
    "        df_dict[\"id\"].append(idx)\n",
    "        df_dict['url'].append(vid['url'])\n",
    "        text = text.encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "        text = \" \".join(text.split())\n",
    "        df_dict['comment'].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('non-hate-comments-with-emoji.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9 (default, Jun 29 2022, 11:45:57) \n[GCC 8.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
