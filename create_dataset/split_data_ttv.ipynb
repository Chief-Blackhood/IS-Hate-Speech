{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import demoji"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from google sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sheet(url):\n",
    "    url_1 = url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\")\n",
    "    data = pd.read_csv(url_1)\n",
    "    return data\n",
    "\n",
    "\n",
    "HATE_SHEET = \"https://docs.google.com/spreadsheets/d/16lxEwKVA_d_g5QRFNcBTyLz_OBPPB3wZdzZu2UnvLWQ/edit#gid=0\"\n",
    "POS_NON_HATE_SHEET = \"https://docs.google.com/spreadsheets/d/16lxEwKVA_d_g5QRFNcBTyLz_OBPPB3wZdzZu2UnvLWQ/edit#gid=1070451623\"\n",
    "NEU_NON_HATE_SHEET = \"https://docs.google.com/spreadsheets/d/16lxEwKVA_d_g5QRFNcBTyLz_OBPPB3wZdzZu2UnvLWQ/edit#gid=497253390\"\n",
    "\n",
    "hate_df = load_sheet(HATE_SHEET)\n",
    "pos_df = load_sheet(POS_NON_HATE_SHEET)\n",
    "neu_df = load_sheet(NEU_NON_HATE_SHEET)\n",
    "non_hate_df = pd.concat([pos_df, neu_df])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_df.drop(\n",
    "    columns=[\n",
    "        \"Title\",\n",
    "        \"Is Video Hateful (Yes / No)\",\n",
    "        \"What Metadata / Information is Required?\",\n",
    "        \"Synthetic or Original?\",\n",
    "        \"Reviewer\",\n",
    "        \"Additional Verification Needed (Yes / No)\",\n",
    "        \"Reason For Additional Verficiation? (Only if YES)\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "hate_df.rename(\n",
    "    columns={\n",
    "        \"Link\": \"url\",\n",
    "        \"Video Category\": \"category\",\n",
    "        \"Comment\": \"comment\",\n",
    "        \"Hate Towards Whom?\": \"hate_towards_whom\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill columns for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_df[\"category\"] = hate_df[\"category\"].str.lower()\n",
    "hate_df[\"label\"] = \"yes\"\n",
    "hate_df.fillna(method=\"ffill\", inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for non hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_hate_df.drop(\n",
    "    columns=[\n",
    "        \"Manual Inspection\",\n",
    "        \"Validator\",\n",
    "        \"scores\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "non_hate_df.rename(\n",
    "    columns={\n",
    "        \"type\": \"category\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "non_hate_df[\"category\"] = non_hate_df[\"category\"].str.lower()\n",
    "non_hate_df[\"hate_towards_whom\"] = \"None\"\n",
    "non_hate_df[\"label\"] = \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([hate_df, non_hate_df])\n",
    "df.drop(columns=['Unnamed: 11'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data according to groups into test and train first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:13<00:00, 741.55it/s]\n"
     ]
    }
   ],
   "source": [
    "groups = df['url']\n",
    "\n",
    "best_state = 0\n",
    "min_diff = 1000000\n",
    "for random_state in tqdm(range(0, 10000)):\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.30, random_state=random_state)\n",
    "\n",
    "    for train_index, test_index in gss.split(df, groups=groups):\n",
    "        df_train = df.iloc[train_index]\n",
    "        df_test = df.iloc[test_index]\n",
    "\n",
    "    try:\n",
    "        count_train = df_train['category'].value_counts().to_dict()\n",
    "        count_test = df_test['category'].value_counts().to_dict()\n",
    "        diff = {k : count_train[k] - count_test[k] * 7 for k in count_train}\n",
    "        value_sum = sum(map(abs, diff.values()))\n",
    "        if value_sum < min_diff:\n",
    "            best_state = random_state\n",
    "            min_diff = value_sum\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8919 1705\n"
     ]
    }
   ],
   "source": [
    "print(best_state, min_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=best_state)\n",
    "\n",
    "for train_index, test_index in gss.split(df, groups=groups):\n",
    "    df_train = df.iloc[train_index]\n",
    "    df_test = df.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1599, 5), (472, 5))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"../data/without_aug/train.csv\", index=False)\n",
    "df_test.to_csv(\"../data/without_aug/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../data/without_aug/test.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all augmented data and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_orig = pd.read_csv(\"../data/without_aug/train.csv\")\n",
    "hate_aug = pd.read_csv(\"../data/with_aug/all_with_uuid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 5)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_orig.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make another split for train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:10<00:00, 935.06it/s]\n"
     ]
    }
   ],
   "source": [
    "groups = hate_orig['url']\n",
    "\n",
    "best_state = 0\n",
    "min_diff = 1000000\n",
    "for random_state in tqdm(range(0, 10000)):\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.35, random_state=random_state)\n",
    "\n",
    "    for train_index, test_index in gss.split(hate_orig, groups=groups):\n",
    "        hate_orig_train = hate_orig.iloc[train_index]\n",
    "        hate_orig_val = hate_orig.iloc[test_index]\n",
    "\n",
    "    try:\n",
    "        count_train = hate_orig_train['category'].value_counts().to_dict()\n",
    "        count_test = hate_orig_val['category'].value_counts().to_dict()\n",
    "        diff = {k : count_train[k] - count_test[k] * 6.5 for k in count_train}\n",
    "        value_sum = sum(map(abs, diff.values()))\n",
    "        if value_sum < min_diff:\n",
    "            best_state = random_state\n",
    "            min_diff = value_sum\n",
    "    except:\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1073 1521.0\n"
     ]
    }
   ],
   "source": [
    "print(best_state, min_diff)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split eval and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.35, random_state=best_state)\n",
    "\n",
    "for train_index, test_index in gss.split(hate_orig, groups=groups):\n",
    "    hate_orig_train = hate_orig.iloc[train_index]\n",
    "    hate_orig_val = hate_orig.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1183, 5), (416, 5))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_orig_train.shape, hate_orig_val.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train aug data only from urls which are not in val and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 121)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_in_val = list(set(hate_orig_val['url'].to_list()))\n",
    "urls_in_test = list(set(df_test['url'].to_list()))\n",
    "len(urls_in_val), len(urls_in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_aug.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((472, 5), (416, 5), (9762, 6))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape, hate_orig_val.shape, hate_aug.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(text):\n",
    "    if text in ['Individual', 'Organisation', 'Location', 'Community', 'None']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = \",\".join([cat for cat in sorted(list(set(text.split()))) if check(cat)])\n",
    "    return text\n",
    "\n",
    "def process_text(text):\n",
    "    if text != text:\n",
    "        return ''\n",
    "    new_text = []\n",
    "\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        t = re.sub(r'http\\S+', '', t)\n",
    "        new_text.append(t)\n",
    "    new_text = \" \".join(new_text)\n",
    "    new_text = demoji.replace_with_desc(new_text, sep=' ')\n",
    "    new_text = re.sub('\\\\s+', ' ', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/8sk6p42d28b00trfq5r8lp0h0000gn/T/ipykernel_4584/1970195426.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hate_orig_val.loc[id, 'comment'] = process_text(row['comment'])\n",
      "/var/folders/63/8sk6p42d28b00trfq5r8lp0h0000gn/T/ipykernel_4584/1970195426.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hate_orig_val.loc[id, 'hate_towards_whom'] = clean(row['hate_towards_whom'])\n",
      "/var/folders/63/8sk6p42d28b00trfq5r8lp0h0000gn/T/ipykernel_4584/1970195426.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hate_orig_val.drop_duplicates(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "for id, row in hate_orig_val.iterrows():\n",
    "    hate_orig_val.loc[id, 'comment'] = process_text(row['comment'])\n",
    "    hate_orig_val.loc[id, 'hate_towards_whom'] = clean(row['hate_towards_whom'])\n",
    "hate_orig_val.drop_duplicates(inplace=True)\n",
    "\n",
    "for id, row in hate_aug.iterrows():\n",
    "    hate_aug.loc[id, 'comment'] = process_text(row['comment'])\n",
    "    hate_aug.loc[id, 'hate_towards_whom'] = clean(row['hate_towards_whom'])\n",
    "hate_aug.drop_duplicates(inplace=True)\n",
    "\n",
    "for id, row in df_test.iterrows():\n",
    "    df_test.loc[id, 'comment'] = process_text(row['comment'])\n",
    "    df_test.loc[id, 'hate_towards_whom'] = clean(row['hate_towards_whom'])\n",
    "df_test.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_with_uuid = pd.merge(df_test, hate_aug, on=['url', 'comment', 'category', 'hate_towards_whom', 'label'], how='left')\n",
    "df_eval_with_uuid = pd.merge(hate_orig_val, hate_aug, on=['url', 'comment', 'category', 'hate_towards_whom', 'label'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9762, 6)\n",
      "(5643, 6)\n"
     ]
    }
   ],
   "source": [
    "print(hate_aug.shape)\n",
    "df_train = hate_aug[~hate_aug['url'].isin(urls_in_val)]\n",
    "df_train = df_train[~df_train['url'].isin(urls_in_test)]\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((472, 6), (416, 6), (5643, 6))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_with_uuid.shape, df_eval_with_uuid.shape, df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_with_uuid.to_csv('../data/with_aug_ttv/eval.csv', index=False)\n",
    "df_train.to_csv('../data/with_aug_ttv/train.csv', index=False)\n",
    "df_test_with_uuid.to_csv(\"../data/with_aug_ttv/test.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all augmented data and clean hate towards whom label for training, validation and test data (Different task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../data/with_aug/test_aug.csv')\n",
    "train_df = pd.read_csv('../data/with_aug/train_aug.csv')\n",
    "all_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_label = ''\n",
    "for id, row in train_df.iterrows():\n",
    "    if str(row['hate_towards_whom']) == 'nan':\n",
    "        train_df.iloc[id]['hate_towards_whom'] = prev_label\n",
    "    else:\n",
    "        prev_label = row['hate_towards_whom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None                       4595\n",
      "Individual                 2414\n",
      "Community                  1631\n",
      "Location                    379\n",
      "Organisation                326\n",
      "Community,Individual        151\n",
      "Community,Location          100\n",
      "Community,Organisation       90\n",
      "Individual,Organisation      39\n",
      "Individual,Location          20\n",
      "Location,Organisation        17\n",
      "Name: hate_towards_whom, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "all_df['hate_towards_whom'] = all_df['hate_towards_whom'].apply(lambda x: clean(x))\n",
    "all_df['comment'] = all_df['comment'].apply(lambda x: process_text(x))\n",
    "all_df.drop_duplicates(inplace=True)\n",
    "\n",
    "all_df.to_csv('../data/with_aug/all.csv', index=False)\n",
    "print(all_df['hate_towards_whom'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>comment</th>\n",
       "      <th>hate_towards_whom</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>because they are bad at geography. And if they...</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>if they don't know where the aliens came from ...</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>if they don't know where the aliens come from ...</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>if they don't know where the aliens come from ...</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Because they are bad in geography, and if the...</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>0:50 that bozzo is wearing a US TShirt as well...</td>\n",
       "      <td>Individual</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>0:50 that bozzo also wears an American t-shirt...</td>\n",
       "      <td>Individual</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1:20 thats the future generation, look at his ...</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1:20 as the future generation, look at its exp...</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>That last white dude was seriously smoking som...</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>that last white dude was smoking something ser...</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>that last white dude was smoking something ser...</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>that last white dude was smoking something really</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>the last white guy was smoking something</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>this last white guy was smoking something</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>that last white man was smoking something</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>that last white guy was smoking something</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>that last white dude was smoking something</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>This last white guy really smoked something.</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>this last white guy smoked something</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>and this last white man really smoked something</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>God save the US. I'll pray for y'all from Indi...</td>\n",
       "      <td>Community,Location</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>I will pray for all of you from Indianapolis, ...</td>\n",
       "      <td>Community,Location</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Well the map looks a hippopotamus in upsidedow...</td>\n",
       "      <td>Location</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>https://www.youtube.com/watch?v=QFOpN957p3s</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Well, the map looks like a hippopotamus in ups...</td>\n",
       "      <td>Location</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              url       category  \\\n",
       "1487  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1488  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1489  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1490  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1491  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1492  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1493  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1494  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1495  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1496  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1497  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1498  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1499  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1500  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1501  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1502  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1503  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1504  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1505  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1506  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1507  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1508  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1509  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1510  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "1511  https://www.youtube.com/watch?v=QFOpN957p3s  entertainment   \n",
       "\n",
       "                                                comment   hate_towards_whom  \\\n",
       "1487  because they are bad at geography. And if they...           Community   \n",
       "1488  if they don't know where the aliens came from ...           Community   \n",
       "1489  if they don't know where the aliens come from ...           Community   \n",
       "1490  if they don't know where the aliens come from ...           Community   \n",
       "1491   Because they are bad in geography, and if the...           Community   \n",
       "1492  0:50 that bozzo is wearing a US TShirt as well...          Individual   \n",
       "1493  0:50 that bozzo also wears an American t-shirt...          Individual   \n",
       "1494  1:20 thats the future generation, look at his ...           Community   \n",
       "1495  1:20 as the future generation, look at its exp...           Community   \n",
       "1496  That last white dude was seriously smoking som...           Community   \n",
       "1497  that last white dude was smoking something ser...           Community   \n",
       "1498  that last white dude was smoking something ser...           Community   \n",
       "1499  that last white dude was smoking something really           Community   \n",
       "1500           the last white guy was smoking something           Community   \n",
       "1501          this last white guy was smoking something           Community   \n",
       "1502          that last white man was smoking something           Community   \n",
       "1503          that last white guy was smoking something           Community   \n",
       "1504         that last white dude was smoking something           Community   \n",
       "1505       This last white guy really smoked something.           Community   \n",
       "1506               this last white guy smoked something           Community   \n",
       "1507    and this last white man really smoked something           Community   \n",
       "1508  God save the US. I'll pray for y'all from Indi...  Community,Location   \n",
       "1509  I will pray for all of you from Indianapolis, ...  Community,Location   \n",
       "1510  Well the map looks a hippopotamus in upsidedow...            Location   \n",
       "1511  Well, the map looks like a hippopotamus in ups...            Location   \n",
       "\n",
       "     label  \n",
       "1487   yes  \n",
       "1488   yes  \n",
       "1489   yes  \n",
       "1490   yes  \n",
       "1491   yes  \n",
       "1492   yes  \n",
       "1493   yes  \n",
       "1494   yes  \n",
       "1495   yes  \n",
       "1496   yes  \n",
       "1497   yes  \n",
       "1498   yes  \n",
       "1499   yes  \n",
       "1500   yes  \n",
       "1501   yes  \n",
       "1502   yes  \n",
       "1503   yes  \n",
       "1504   yes  \n",
       "1505   yes  \n",
       "1506   yes  \n",
       "1507   yes  \n",
       "1508   yes  \n",
       "1509   yes  \n",
       "1510   yes  \n",
       "1511   yes  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df[all_df['url'] == 'https://www.youtube.com/watch?v=QFOpN957p3s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hate_aug' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/chief-blackhood/Desktop/IS/create_dataset/split_data_ttv.ipynb Cell 40\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chief-blackhood/Desktop/IS/create_dataset/split_data_ttv.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m hate_aug[hate_aug[\u001b[39m'\u001b[39m\u001b[39murl\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://www.youtube.com/watch?v=QFOpN957p3s\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hate_aug' is not defined"
     ]
    }
   ],
   "source": [
    "hate_aug[hate_aug['url'] == 'https://www.youtube.com/watch?v=QFOpN957p3s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_csv('../data/with_aug/all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
