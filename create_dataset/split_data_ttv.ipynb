{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import demoji"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from google sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sheet(url):\n",
    "    url_1 = url.replace(\"/edit#gid=\", \"/export?format=csv&gid=\")\n",
    "    data = pd.read_csv(url_1)\n",
    "    return data\n",
    "\n",
    "\n",
    "HATE_SHEET = \"https://docs.google.com/spreadsheets/d/16lxEwKVA_d_g5QRFNcBTyLz_OBPPB3wZdzZu2UnvLWQ/edit#gid=0\"\n",
    "POS_NON_HATE_SHEET = \"https://docs.google.com/spreadsheets/d/16lxEwKVA_d_g5QRFNcBTyLz_OBPPB3wZdzZu2UnvLWQ/edit#gid=1070451623\"\n",
    "NEU_NON_HATE_SHEET = \"https://docs.google.com/spreadsheets/d/16lxEwKVA_d_g5QRFNcBTyLz_OBPPB3wZdzZu2UnvLWQ/edit#gid=497253390\"\n",
    "\n",
    "hate_df = load_sheet(HATE_SHEET)\n",
    "pos_df = load_sheet(POS_NON_HATE_SHEET)\n",
    "neu_df = load_sheet(NEU_NON_HATE_SHEET)\n",
    "non_hate_df = pd.concat([pos_df, neu_df])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_df.drop(\n",
    "    columns=[\n",
    "        \"Title\",\n",
    "        \"Is Video Hateful (Yes / No)\",\n",
    "        \"What Metadata / Information is Required?\",\n",
    "        \"Synthetic or Original?\",\n",
    "        \"Reviewer\",\n",
    "        \"Additional Verification Needed (Yes / No)\",\n",
    "        \"Reason For Additional Verficiation? (Only if YES)\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "hate_df.rename(\n",
    "    columns={\n",
    "        \"Link\": \"url\",\n",
    "        \"Video Category\": \"category\",\n",
    "        \"Comment\": \"comment\",\n",
    "        \"Hate Towards Whom?\": \"hate_towards_whom\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill columns for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_df[\"category\"] = hate_df[\"category\"].str.lower()\n",
    "hate_df[\"label\"] = \"yes\"\n",
    "hate_df.fillna(method=\"ffill\", inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for non hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_hate_df.drop(\n",
    "    columns=[\n",
    "        \"Manual Inspection\",\n",
    "        \"Validator\",\n",
    "        \"scores\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "non_hate_df.rename(\n",
    "    columns={\n",
    "        \"type\": \"category\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "non_hate_df[\"category\"] = non_hate_df[\"category\"].str.lower()\n",
    "non_hate_df[\"hate_towards_whom\"] = \"None\"\n",
    "non_hate_df[\"label\"] = \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([hate_df, non_hate_df])\n",
    "df.drop(columns=['Unnamed: 11'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data according to groups into test and train first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:22<00:00, 447.38it/s]\n"
     ]
    }
   ],
   "source": [
    "groups = df['url']\n",
    "\n",
    "best_state = 0\n",
    "min_diff = 1000000\n",
    "for random_state in tqdm(range(0, 10000)):\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.30, random_state=random_state)\n",
    "\n",
    "    for train_index, test_index in gss.split(df, groups=groups):\n",
    "        df_train = df.iloc[train_index]\n",
    "        df_test = df.iloc[test_index]\n",
    "\n",
    "    try:\n",
    "        count_train = df_train['category'].value_counts().to_dict()\n",
    "        count_test = df_test['category'].value_counts().to_dict()\n",
    "        diff = {k : count_train[k] - count_test[k] * 7 for k in count_train}\n",
    "        value_sum = sum(map(abs, diff.values()))\n",
    "        if value_sum < min_diff:\n",
    "            best_state = random_state\n",
    "            min_diff = value_sum\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8919 1705\n"
     ]
    }
   ],
   "source": [
    "print(best_state, min_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=best_state)\n",
    "\n",
    "for train_index, test_index in gss.split(df, groups=groups):\n",
    "    df_train = df.iloc[train_index]\n",
    "    df_test = df.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1599, 5), (472, 5))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"../data/without_aug/train.csv\", index=False)\n",
    "df_test.to_csv(\"../data/without_aug/test.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all augmented data and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_orig = pd.read_csv(\"../data/without_aug/train.csv\")\n",
    "hate_aug = pd.read_csv(\"../data/with_aug/all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 5)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_orig.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make another split for train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:17<00:00, 562.86it/s]\n"
     ]
    }
   ],
   "source": [
    "groups = hate_orig['url']\n",
    "\n",
    "best_state = 0\n",
    "min_diff = 1000000\n",
    "for random_state in tqdm(range(0, 10000)):\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.35, random_state=random_state)\n",
    "\n",
    "    for train_index, test_index in gss.split(hate_orig, groups=groups):\n",
    "        hate_orig_train = hate_orig.iloc[train_index]\n",
    "        hate_orig_val = hate_orig.iloc[test_index]\n",
    "\n",
    "    try:\n",
    "        count_train = hate_orig_train['category'].value_counts().to_dict()\n",
    "        count_test = hate_orig_val['category'].value_counts().to_dict()\n",
    "        diff = {k : count_train[k] - count_test[k] * 6.5 for k in count_train}\n",
    "        value_sum = sum(map(abs, diff.values()))\n",
    "        if value_sum < min_diff:\n",
    "            best_state = random_state\n",
    "            min_diff = value_sum\n",
    "    except:\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1073 1521.0\n"
     ]
    }
   ],
   "source": [
    "print(best_state, min_diff)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split eval and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.35, random_state=best_state)\n",
    "\n",
    "for train_index, test_index in gss.split(hate_orig, groups=groups):\n",
    "    hate_orig_train = hate_orig.iloc[train_index]\n",
    "    hate_orig_val = hate_orig.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1183, 5), (416, 5))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_orig_train.shape, hate_orig_val.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train aug data only from urls which are not in val and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 121)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_in_val = list(set(hate_orig_val['url'].to_list()))\n",
    "urls_in_test = list(set(df_test['url'].to_list()))\n",
    "len(urls_in_val), len(urls_in_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4948, 5)\n",
      "(2866, 5)\n"
     ]
    }
   ],
   "source": [
    "print(hate_aug.shape)\n",
    "hate_aug = hate_aug[~hate_aug['url'].isin(urls_in_val)]\n",
    "hate_aug = hate_aug[~hate_aug['url'].isin(urls_in_test)]\n",
    "print(hate_aug.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(text):\n",
    "    if text in ['Individual', 'Organisation', 'Location', 'Community', 'None']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    text = \",\".join([cat for cat in sorted(list(set(text.split()))) if check(cat)])\n",
    "    return text\n",
    "\n",
    "def process_text(text):\n",
    "    if text != text:\n",
    "        return ''\n",
    "    new_text = []\n",
    "\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        t = re.sub(r'http\\S+', '', t)\n",
    "        new_text.append(t)\n",
    "    new_text = \" \".join(new_text)\n",
    "    new_text = demoji.replace_with_desc(new_text, sep=' ')\n",
    "    new_text = re.sub('\\\\s+', ' ', new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/63/8sk6p42d28b00trfq5r8lp0h0000gn/T/ipykernel_3421/165573617.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hate_orig_val['hate_towards_whom'] = hate_orig_val['hate_towards_whom'].apply(lambda x: clean(x))\n",
      "/var/folders/63/8sk6p42d28b00trfq5r8lp0h0000gn/T/ipykernel_3421/165573617.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hate_orig_val['comment'] = hate_orig_val['comment'].apply(lambda x: process_text(x))\n",
      "/var/folders/63/8sk6p42d28b00trfq5r8lp0h0000gn/T/ipykernel_3421/165573617.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hate_orig_val.drop_duplicates(inplace=True)\n",
      "/var/folders/63/8sk6p42d28b00trfq5r8lp0h0000gn/T/ipykernel_3421/165573617.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['hate_towards_whom'] = df_test['hate_towards_whom'].apply(lambda x: clean(x))\n",
      "/var/folders/63/8sk6p42d28b00trfq5r8lp0h0000gn/T/ipykernel_3421/165573617.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['comment'] = df_test['comment'].apply(lambda x: process_text(x))\n",
      "/var/folders/63/8sk6p42d28b00trfq5r8lp0h0000gn/T/ipykernel_3421/165573617.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test.drop_duplicates(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "hate_orig_val['hate_towards_whom'] = hate_orig_val['hate_towards_whom'].apply(lambda x: clean(x))\n",
    "hate_orig_val['comment'] = hate_orig_val['comment'].apply(lambda x: process_text(x))\n",
    "hate_orig_val.drop_duplicates(inplace=True)\n",
    "\n",
    "hate_aug['hate_towards_whom'] = hate_aug['hate_towards_whom'].apply(lambda x: clean(x))\n",
    "hate_aug['comment'] = hate_aug['comment'].apply(lambda x: process_text(x))\n",
    "hate_aug.drop_duplicates(inplace=True)\n",
    "\n",
    "df_test['hate_towards_whom'] = df_test['hate_towards_whom'].apply(lambda x: clean(x))\n",
    "df_test['comment'] = df_test['comment'].apply(lambda x: process_text(x))\n",
    "df_test.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_orig_val.to_csv('../data/with_aug_ttv/eval.csv', index=False)\n",
    "hate_aug.to_csv('../data/with_aug_ttv/train.csv', index=False)\n",
    "df_test.to_csv(\"../data/with_aug_ttv/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>comment</th>\n",
       "      <th>hate_towards_whom</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.bitchute.com/video/UNB7iw1M7Uco/</td>\n",
       "      <td>fashion &amp; beauty</td>\n",
       "      <td>you cannot mix Wild Chimps into the general po...</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.bitchute.com/video/UNB7iw1M7Uco/</td>\n",
       "      <td>fashion &amp; beauty</td>\n",
       "      <td>there is no way to mix wild chimps with the ge...</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.bitchute.com/video/UNB7iw1M7Uco/</td>\n",
       "      <td>fashion &amp; beauty</td>\n",
       "      <td>there's no way to mix wild chimps with the gen...</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.bitchute.com/video/UNB7iw1M7Uco/</td>\n",
       "      <td>fashion &amp; beauty</td>\n",
       "      <td>you can't mix wild chimps with the general pop...</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.bitchute.com/video/UNB7iw1M7Uco/</td>\n",
       "      <td>fashion &amp; beauty</td>\n",
       "      <td>keep white and black separate</td>\n",
       "      <td>Community</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             url          category  \\\n",
       "10  https://www.bitchute.com/video/UNB7iw1M7Uco/  fashion & beauty   \n",
       "11  https://www.bitchute.com/video/UNB7iw1M7Uco/  fashion & beauty   \n",
       "12  https://www.bitchute.com/video/UNB7iw1M7Uco/  fashion & beauty   \n",
       "13  https://www.bitchute.com/video/UNB7iw1M7Uco/  fashion & beauty   \n",
       "14  https://www.bitchute.com/video/UNB7iw1M7Uco/  fashion & beauty   \n",
       "\n",
       "                                              comment hate_towards_whom label  \n",
       "10  you cannot mix Wild Chimps into the general po...         Community   yes  \n",
       "11  there is no way to mix wild chimps with the ge...         Community   yes  \n",
       "12  there's no way to mix wild chimps with the gen...         Community   yes  \n",
       "13  you can't mix wild chimps with the general pop...         Community   yes  \n",
       "14                      keep white and black separate         Community   yes  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_aug"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all augmented data and clean hate towards whom label for training, validation and test data (Different task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../data/with_aug/test_aug.csv')\n",
    "train_df = pd.read_csv('../data/with_aug/train_aug.csv')\n",
    "eval_df = pd.read_csv('../data/with_aug_ttv/eval.csv')\n",
    "all_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_label = ''\n",
    "for id, row in train_df.iterrows():\n",
    "    if str(row['hate_towards_whom']) == 'nan':\n",
    "        train_df.iloc[id]['hate_towards_whom'] = prev_label\n",
    "    else:\n",
    "        prev_label = row['hate_towards_whom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None                       2354\n",
      "Individual                 1205\n",
      "Community                   827\n",
      "Location                    201\n",
      "Organisation                157\n",
      "Community,Individual         81\n",
      "Community,Location           47\n",
      "Community,Organisation       41\n",
      "Individual,Organisation      17\n",
      "Location,Organisation         9\n",
      "Individual,Location           9\n",
      "Name: hate_towards_whom, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "all_df['hate_towards_whom'] = all_df['hate_towards_whom'].apply(lambda x: clean(x))\n",
    "all_df['comment'] = all_df['comment'].apply(lambda x: process_text(x))\n",
    "all_df.drop_duplicates(inplace=True)\n",
    "\n",
    "all_df.to_csv('../data/with_aug/all.csv', index=False)\n",
    "print(all_df['hate_towards_whom'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
