{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy\n",
    "\n",
    "class MultiTaskLoss(nn.Module):\n",
    "  def __init__(self, n_tasks, reduction='none'):\n",
    "    super(MultiTaskLoss, self).__init__()\n",
    "    self.n_tasks = n_tasks\n",
    "    self.log_vars = nn.Parameter(torch.zeros(self.n_tasks))\n",
    "    self.reduction = reduction\n",
    "\n",
    "  def forward(self, losses):\n",
    "    dtype = losses.dtype\n",
    "    device = losses.device\n",
    "    stds = (torch.exp(self.log_vars)**(1/2)).to(device).to(dtype)\n",
    "    multi_task_losses = (1 / (stds ** 2)) * losses + torch.log(stds)\n",
    "\n",
    "    if self.reduction == 'sum':\n",
    "      multi_task_losses = multi_task_losses.sum()\n",
    "    if self.reduction == 'mean':\n",
    "      multi_task_losses = multi_task_losses.mean()\n",
    "\n",
    "    return multi_task_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESIZE = (128, 171)\n",
    "MEAN = [0.43216, 0.394666, 0.37645]\n",
    "STD = [0.22803, 0.22145, 0.216989]\n",
    "CROP_SIZE = 112\n",
    "NUM_FRAMES = 10\n",
    "NCHANNELS = 3\n",
    "NTH_SECOND = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from opencv_transforms.transforms import (\n",
    "    Compose, Normalize, Resize, CenterCrop, ToTensor\n",
    ")\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "class HateSpeechData(data.Dataset):\n",
    "\n",
    "    def __init__(self, args, phase):\n",
    "        self.mapping = {\"Organisation\": 0, \"Location\": 1, \"Individual\": 2, \"Community\": 3, \"None\": 4}\n",
    "        self.args = args\n",
    "        if phase == 'train':\n",
    "            self.comments = self.load_comments(self.args[\"train_question_file\"])\n",
    "        elif phase == 'validation':\n",
    "            self.comments = self.load_comments(self.args[\"validation_question_file\"])\n",
    "        else:\n",
    "            self.comments = self.load_comments(self.args[\"test_question_file\"])\n",
    "\n",
    "        self.comments[['videoID', 'source']] = self.comments['url'].apply(lambda x: self.parse_urls(x))\n",
    "        \n",
    "        if args[\"add_video\"]:\n",
    "            self.transform = Compose(\n",
    "                [\n",
    "                    Resize(RESIZE),\n",
    "                    CenterCrop(CROP_SIZE),\n",
    "                    ToTensor(),\n",
    "                    Normalize(MEAN, STD)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        if args[\"add_other_comments\"]:\n",
    "            self.other_comments_data = self.load_metadata(self.args[\"other_comments_path\"])\n",
    "            self.comments = pd.merge(self.comments, self.other_comments_data, how='left', on=['url', 'comment'])\n",
    "        if args[\"add_title\"] or args[\"add_description\"] or args[\"add_transcription\"]:\n",
    "            self.metadata = self.load_metadata(args[\"metadata_path\"])\n",
    "            self.metadata = self.metadata.replace(np.nan, '', regex=True)\n",
    "            if \"bert\" in self.args[\"model\"]:\n",
    "                if self.args[\"desc_keyphrase_extract\"]:\n",
    "                    self.metadata['desc'] = self.metadata['key_phrases_desc_bert']\n",
    "                if self.args[\"transcript_keyphrase_extract\"]:\n",
    "                    self.metadata['transcript'] = self.metadata['key_phrases_transcript_bert']\n",
    "            elif \"longformer\" in self.args[\"model\"]:\n",
    "                if self.args[\"desc_keyphrase_extract\"]:\n",
    "                    self.metadata['desc'] = self.metadata['key_phrases_desc_long']\n",
    "                if self.args[\"transcript_keyphrase_extract\"]:\n",
    "                    self.metadata['transcript'] = self.metadata['key_phrases_transcript_long']\n",
    "            self.comments = pd.merge(self.comments, self.metadata, how='left', on='url')\n",
    "        self.comments = self.comments.replace(np.nan, '', regex=True)\n",
    "        self.comments.drop_duplicates(inplace=True)\n",
    "        if self.args[\"remove_none\"]:\n",
    "            self.comments = self.comments[self.comments['label'] == True]\n",
    "        self.comments = self.comments.reset_index(drop=True)\n",
    "\n",
    "    def load_metadata(self, filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        return df\n",
    "    \n",
    "    def parse_urls(self, url):\n",
    "        videoID = \"\"\n",
    "        source = \"\"\n",
    "        if \"youtube\" in url:\n",
    "            source = \"youtube\"\n",
    "            videoID = url.split(\"watch?v=\")[1].split(\"_channel=\")[0].split(\"&t=\")[0].split(\"&lc=\")[0].split(\"&ab\")[0].split(\"&\")[0]\n",
    "            if len(videoID) != 11:\n",
    "                print(videoID)\n",
    "            assert len(videoID) == 11\n",
    "\n",
    "        elif \"bitchute\" in url:\n",
    "            source = \"bitchute\"\n",
    "            videoID = url.split(\"/\")[-2]\n",
    "    \n",
    "        return  pd.Series({'videoID': videoID, 'source': source})\n",
    "\n",
    "    def load_comments(self, filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        df['label'] = df['label'].apply(lambda x: int(x == 'yes'))\n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment = self.comments['comment'][index] if self.args[\"add_comment\"] else ''\n",
    "        title = self.comments['title'][index] if self.args[\"add_title\"] else ''\n",
    "        desc = self.comments['desc'][index] if self.args[\"add_description\"] else ''\n",
    "        transcript = self.comments['transcript'][index] if self.args[\"add_transcription\"] else ''\n",
    "        other_comment = self.comments['key_phrases_other_comments'][index] if self.args[\"add_other_comments\"] else ''\n",
    "\n",
    "        frame_data = torch.zeros(NCHANNELS, 1, CROP_SIZE, CROP_SIZE)\n",
    "        if self.args[\"add_video\"]:\n",
    "            frame_data = []\n",
    "            filename = os.path.join(self.args[\"video_path, self.comments['source'][index], self.comments['videoID'][index]\"])\n",
    "            vidcap = cv2.VideoCapture(f\"{filename}.mp4\")\n",
    "            frame_rate = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "            frames_step = frame_rate * NTH_SECOND\n",
    "\n",
    "            num_frames = 0\n",
    "            while vidcap.isOpened():\n",
    "                ret, image = vidcap.read()\n",
    "                if ret:\n",
    "                    image = self.transform(image)\n",
    "                    image = image[None, ...]\n",
    "                    frame_data.append(image)\n",
    "                    num_frames += 1\n",
    "                    vidcap.set(cv2.CAP_PROP_POS_FRAMES, num_frames * frames_step)\n",
    "                else:\n",
    "                    vidcap.release()\n",
    "                    break\n",
    "\n",
    "            frame_data = torch.cat(frame_data) # Number of frames, channels, image width, image height\n",
    "            frame_data = torch.movedim(frame_data, 1, 0) # channels, Number of frames, image width, image height\n",
    "            \n",
    "        target_multilabel = np.zeros(5, dtype=float)\n",
    "        if self.args[\"remove_none\"]:\n",
    "            target_multilabel = np.zeros(4, dtype=float) \n",
    "        labels = self.comments['hate_towards_whom'][index].split(',')\n",
    "        for label in labels:\n",
    "            label = label.strip()\n",
    "            if label == '' or len(label) == 0:\n",
    "              print(comment, index, labels, title)\n",
    "            target_multilabel[self.mapping[label]] = 1\n",
    "        target_multilabel = torch.FloatTensor(target_multilabel)\n",
    "        target_binary = torch.FloatTensor([self.comments['label'][index]])\n",
    "        \n",
    "        return comment, title, desc, transcript, other_comment, frame_data, target_binary, target_multilabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class LFEmbeddingModule(nn.Module):\n",
    "    def __init__(self, args, device):\n",
    "        super(LFEmbeddingModule, self).__init__()\n",
    "        self.args = args\n",
    "        if 'longformer' in self.args[\"model\"]:\n",
    "            self.lf_model = LongformerModel.from_pretrained(self.args[\"model\"], output_hidden_states=True).to(device)\n",
    "            self.lf_tokenizer = LongformerTokenizer.from_pretrained(self.args[\"model\"])\n",
    "        else:\n",
    "            self.lf_model = BertModel.from_pretrained(self.args[\"model\"], output_hidden_states=True).to(device)\n",
    "            self.lf_tokenizer = BertTokenizer.from_pretrained(self.args[\"model\"])\n",
    "\n",
    "        self.device = device\n",
    "        modules = [self.lf_model.embeddings, *self.lf_model.encoder.layer[:self.args[\"freeze_lf_layers\"]]]\n",
    "        for module in modules:\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        \n",
    "    def get_embeddings(self, comments, titles, descriptions, transcripts, other_comments):\n",
    "        indexed_cs = []\n",
    "        max_len_total = self.args[\"max_len\"]\n",
    "        max_len_title = self.args[\"title_token_count\"]\n",
    "        max_len_desc = self.args[\"desc_token_count\"]\n",
    "        max_len_trans = self.args[\"transcript_token_count\"]\n",
    "        max_len_other_comments = self.args[\"other_comments_token_count\"]\n",
    "        padding = 'max_length' if self.args[\"pad_metadata\"] else False\n",
    "        for comment, title, desc, transcript, other_comment in zip(comments, titles, descriptions, transcripts, other_comments):\n",
    "            enc_c = []\n",
    "            if self.args[\"add_comment\"]:\n",
    "                enc_c = self.lf_tokenizer.encode_plus(comment, max_length=max_len_total, padding=False, truncation=True)['input_ids']\n",
    "            if self.args[\"add_title\"]:\n",
    "                enc_t = self.lf_tokenizer.encode_plus(title, max_length=max_len_title, padding=padding, truncation=True)['input_ids']\n",
    "                if len(enc_c) == 0:\n",
    "                    enc_c.extend(enc_t)\n",
    "                else:\n",
    "                    enc_c.extend(enc_t[1:])\n",
    "            if self.args[\"add_description\"]:\n",
    "                enc_d = self.lf_tokenizer.encode_plus(desc, max_length=max_len_desc, padding=padding, truncation=True)['input_ids']\n",
    "                if len(enc_c) == 0:\n",
    "                    enc_c.extend(enc_d)\n",
    "                else:\n",
    "                    enc_c.extend(enc_d[1:])\n",
    "            if self.args[\"add_transcription\"]:\n",
    "                enc_tr = self.lf_tokenizer.encode_plus(transcript, max_length=max_len_trans, padding=padding, truncation=True)['input_ids']\n",
    "                if len(enc_c) == 0:\n",
    "                    enc_c.extend(enc_tr)\n",
    "                else:\n",
    "                    enc_c.extend(enc_tr[1:])\n",
    "            if self.args[\"add_other_comments\"]:\n",
    "                enc_oc = self.lf_tokenizer.encode_plus(other_comment, max_length=max_len_other_comments, padding=padding, truncation=True)['input_ids']\n",
    "                if len(enc_c) == 0:\n",
    "                    enc_c.extend(enc_oc)\n",
    "                else:\n",
    "                    enc_c.extend(enc_oc[1:])\n",
    "            enc_c = enc_c[:max_len_total]\n",
    "            enc_c.extend((max_len_total - len(enc_c))*[self.lf_tokenizer.pad_token_id])\n",
    "            indexed_cs.append(enc_c)\n",
    "        indexed_cs = torch.tensor(indexed_cs).to(self.device)\n",
    "        embedding = self.lf_model(indexed_cs)\n",
    "        return embedding\n",
    "\n",
    "class VisionModule(nn.Module):\n",
    "    def __init__(self, args, device):\n",
    "        super(VisionModule, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        pretrained_model = torchvision.models.video.r3d_18(pretrained=True)\n",
    "        self.model = torch.nn.Sequential(*(list(pretrained_model.children())[:-1])).to(device)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def get_embeddings(self, frames):\n",
    "        frames = frames.to(self.device)\n",
    "        vision_embedding = []\n",
    "        for frame in list(frames):\n",
    "            sum_frame = frame.sum(dim=(0, 2, 3))\n",
    "            num_zero_indices = (sum_frame == 0).nonzero().flatten().shape[0]\n",
    "            final_frame = frame[:, :(-num_zero_indices if num_zero_indices else frame.shape[1]), :, :]\n",
    "            _vis_emb = self.model(final_frame[None, ...])\n",
    "            _vis_emb = torch.flatten(_vis_emb, start_dim=1)\n",
    "            vision_embedding.append(_vis_emb)\n",
    "\n",
    "        vision_embedding = torch.cat(vision_embedding)\n",
    "        return vision_embedding\n",
    "\n",
    "        \n",
    "class CommentModel(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CommentModel, self).__init__()\n",
    "        self.args = args\n",
    "        if 'base' in self.args[\"model\"]:\n",
    "            self.fc_size = 768\n",
    "        else:\n",
    "            self.fc_size = 1024\n",
    "        if self.args[\"add_video\"]:\n",
    "            self.fc_size += 512\n",
    "        output_size = 5\n",
    "        if self.args[\"remove_none\"]:\n",
    "            output_size = 4\n",
    "        self.fc_multilabel = nn.Sequential(\n",
    "            nn.Linear(self.fc_size, output_size),\n",
    "        )\n",
    "        self.fc_binary = nn.Sequential(\n",
    "            nn.Linear(self.fc_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, text_emb, vision_emb):\n",
    "        inp_emb = text_emb\n",
    "        if self.args[\"add_video\"]:\n",
    "            inp_emb = torch.cat([inp_emb, vision_emb], dim = 1)\n",
    "        \n",
    "        out = [self.fc_multilabel(inp_emb), self.fc_binary(inp_emb)]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, labels):\n",
    "    return np.sum(pred == labels)/pred.shape[0]\n",
    "\n",
    "def save_checkpoint(state, args, name, filename='checkpoint.pth.tar', is_best=False):\n",
    "    # torch.save(state, filename)\n",
    "    if is_best:\n",
    "        lf_filename = os.path.join(args[\"work_dir\"], 'lf_model_' + str(name) +'.pth.tar')\n",
    "        comment_filename = os.path.join(args[\"work_dir\"], 'comment_model_' + str(name) +'.pth.tar')\n",
    "        best_filename = lf_filename if 'lf_model' in filename else comment_filename\n",
    "        torch.save(state, best_filename)\n",
    "        # shutil.copyfile(filename, best_filename)\n",
    "\n",
    "\n",
    "def load_weights(name, lf_model, comment_model, args):\n",
    "    lf_checkpoint = os.path.join(args[\"work_dir\"], 'lf_model_' + str(name)+'.pth.tar')\n",
    "    comment_checkpoint = os.path.join(args[\"work_dir\"], 'comment_model_' + str(name)+'.pth.tar')\n",
    "    \n",
    "    lf_model.lf_model.load_state_dict(torch.load(lf_checkpoint, map_location=torch.device('mps'))['state_dict'])\n",
    "    comment_model.load_state_dict(torch.load(comment_checkpoint, map_location=torch.device('mps'))['state_dict'])\n",
    "    return\n",
    "\n",
    "def collate_fn(batch):\n",
    "    comments, titles, descriptions, transcriptions, other_comments, frames, label_binary, label_multilabel = zip(*batch)\n",
    "\n",
    "    max_frames = max([image.size(1) for image in frames])\n",
    "    frames = torch.tensor(np.array([F.pad(image, [0, 0, 0, 0, 0, max_frames - image.size(1)]).numpy() for image in frames]))\n",
    "    label_binary = torch.tensor(label_binary).reshape(-1, 1)\n",
    "    label_multilabel = torch.stack(list(label_multilabel), dim=0)\n",
    "    return [comments, titles, descriptions, transcriptions, other_comments, frames, label_binary, label_multilabel]\n",
    "\n",
    "\n",
    "def get_data_loaders(args, phase):\n",
    "    shuffle = True if phase == \"train\" else False\n",
    "    data = HateSpeechData(args, phase)\n",
    "    dataloader = DataLoader(data, collate_fn=collate_fn, batch_size=args[\"batch_size\"], shuffle=shuffle, num_workers=args[\"num_workers\"])\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, epoch, phase, device, criterions, optimizer, lf_model, vision_model, comment_model, multitaskloss_instance, args):\n",
    "    \n",
    "    lf_model.lf_model.train()\n",
    "    vision_model.model.eval()\n",
    "    comment_model.train()\n",
    "    multitaskloss_instance.train()\n",
    "\n",
    "    \n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    for itr, (comment, title, description, transcription, other_comments, frames, label_binary, label_multilabel) in enumerate(train_loader):\n",
    "        label_binary = label_binary.to(device)\n",
    "        label_multilabel = label_multilabel.to(device)\n",
    "\n",
    "        vis_emb = None\n",
    "        if args[\"add_video\"]:\n",
    "            vis_emb = vision_model.get_embeddings(frames)\n",
    "        \n",
    "        output = comment_model(lf_model.get_embeddings(comment, title, description, transcription, other_comments)[1], vis_emb)\n",
    "\n",
    "        loss_multilabel = criterions[0](output[0], label_multilabel)        \n",
    "        loss_binary = criterions[1](output[1], label_binary)      \n",
    "\n",
    "        losses_stack = torch.stack([loss_multilabel, loss_binary])\n",
    "        multitaskloss = multitaskloss_instance(losses_stack)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        multitaskloss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        output_binary = np.round(output[1].detach().cpu().numpy())\n",
    "        label_binary = np.round(label_binary.detach().cpu().numpy())\n",
    "    \n",
    "        acc = accuracy(output_binary, label_binary)\n",
    "        acces.update(acc, args[\"batch_size\"])\n",
    "        \n",
    "        losses.update(multitaskloss.data.item(), args[\"batch_size\"])\n",
    "\n",
    "        if itr % 25 == 0:\n",
    "            print(phase + ' Epoch-{:<3d} Iter-{:<3d}/{:<3d}\\t'\n",
    "                'loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                'accu {acc.val:.3f} ({acc.avg:.3f})\\t'.format(\n",
    "                epoch, itr, len(train_loader), loss=losses, acc=acces))\n",
    "\n",
    "    return losses.avg, acces.avg\n",
    "        \n",
    "def eval_one_epoch(test_loader, epoch, phase, device, criterions, lf_model, vision_model, comment_model, multitaskloss_instance, args):\n",
    "\n",
    "    lf_model.lf_model.eval()\n",
    "    vision_model.model.eval()\n",
    "    comment_model.eval()\n",
    "    multitaskloss_instance.eval()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    acces = AverageMeter()\n",
    "    \n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for itr, (comment, title, description, transcription, other_comments, frames, label_binary, label_multilabel) in enumerate(test_loader):\n",
    "            label_binary = label_binary.to(device)\n",
    "            label_multilabel = label_multilabel.to(device)\n",
    "\n",
    "            vis_emb = None\n",
    "            if args[\"add_video\"]:\n",
    "                vis_emb = vision_model.get_embeddings(frames)\n",
    "\n",
    "            output = comment_model(lf_model.get_embeddings(comment, title, description, transcription, other_comments)[1], vis_emb)\n",
    "\n",
    "            loss_multilabel = criterions[0](output[0], label_multilabel)        \n",
    "            loss_binary = criterions[1](output[1], label_binary)      \n",
    "\n",
    "            losses_stack = torch.stack([loss_multilabel, loss_binary])\n",
    "            multitaskloss = multitaskloss_instance(losses_stack)\n",
    "\n",
    "            output_binary = output[1].detach().cpu().numpy()\n",
    "            output_multilabel = output[0].detach().cpu().numpy()\n",
    "            # print(np.sum(np.exp(output_multilabel), axis=-1))\n",
    "            output_multilabel = np.exp(output_multilabel) / np.sum(np.exp(output_multilabel), axis=-1)[:, np.newaxis]\n",
    "\n",
    "            label_binary = np.round(label_binary.detach().cpu().numpy())\n",
    "            label_multilabel = np.round(label_multilabel.detach().cpu().numpy())\n",
    "        \n",
    "            acc = accuracy(np.round(output_binary), label_binary)\n",
    "            acces.update(acc, args[\"batch_size\"])\n",
    "\n",
    "            losses.update(multitaskloss.data.item(), args[\"batch_size\"])\n",
    "\n",
    "            final_outputs = np.c_[output_multilabel, output_binary]\n",
    "            final_labels = np.c_[label_multilabel, label_binary]\n",
    "            # print(final_outputs.shape)\n",
    "\n",
    "            preds.extend([final_outputs.tolist()])\n",
    "            labels.extend([final_labels.tolist()])\n",
    "        \n",
    "\n",
    "            if itr % 25 == 0:\n",
    "                print(phase + ' Epoch-{:<3d} Iter-{:<3d}/{:<3d}\\t'\n",
    "                    'loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'accu {acc.val:.3f} ({acc.avg:.3f})\\t'.format(\n",
    "                    epoch, itr, len(test_loader), loss=losses, acc=acces))     \n",
    "\n",
    "    return losses.avg, acces.avg, preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():\n",
    "  args = {}\n",
    "  args[\"work_dir\"]='../..//models/27May2023/'\n",
    "  args[\"train_question_file\"]='../../data/with_aug_ttv/train.csv'\n",
    "  args[\"validation_question_file\"]='../../data/with_aug_ttv/eval.csv'\n",
    "  args[\"test_question_file\"]='../../data/with_aug_ttv/test.csv'\n",
    "  args[\"batch_size\"]=8\n",
    "  args[\"model\"]='bert-large-cased'\n",
    "  args[\"lr\"]=0.0003\n",
    "  args[\"num_workers\"]=0\n",
    "  args[\"max_epochs\"]=0\n",
    "  args[\"max_len\"]=512\n",
    "  args[\"gpu\"]='0'\n",
    "  args[\"freeze_lf_layers\"]=23\n",
    "  args[\"metadata_path\"]='../../data/extra_data_trans.csv'\n",
    "  args[\"pad_metadata\"]=True\n",
    "  args[\"add_comment\"]=True\n",
    "  args[\"add_title\"]=False\n",
    "  args[\"title_token_count\"]=50\n",
    "  args[\"add_description\"]=False\n",
    "  args[\"desc_keyphrase_extract\"]=False\n",
    "  args[\"desc_token_count\"]=100\n",
    "  args[\"add_transcription\"]=False\n",
    "  args[\"transcript_keyphrase_extract\"]=False\n",
    "  args[\"transcript_token_count\"]=300\n",
    "  args[\"other_comments_path\"]='../../data/extra_data_other_comments.csv'\n",
    "  args[\"add_other_comments\"]=False\n",
    "  args[\"other_comments_token_count\"]=4096\n",
    "  args[\"add_video\"]=False\n",
    "  args[\"video_path\"]='../../data/videos/'\n",
    "  args[\"multitask\"]=True\n",
    "  args[\"remove_none\"]=False\n",
    "  return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_args = get_params()\n",
    "model_args = {}\n",
    "new_args = common_args.copy()\n",
    "# new_args.update({\"model\": \"allenai/longformer-base-4096\", \"max_len\": 4096, \"freeze_lf_layers\": 11, \"add_title\": True, \"title_token_count\": 80, \"add_description\": True, \"desc_keyphrase_extract\": True, \"desc_token_count\": 400, \"add_transcription\": True, \"transcript_keyphrase_extract\": True, \"transcript_token_count\": 2200, \"add_other_comments\": True})\n",
    "# model_args[\"revived-dream-83\"] = new_args\n",
    "# new_args = common_args.copy()\n",
    "# new_args.update({\"model\": \"allenai/longformer-base-4096\", \"max_len\": 4096, \"freeze_lf_layers\": 11, \"add_title\": True, \"title_token_count\": 100, \"add_description\": True, \"desc_keyphrase_extract\": True, \"desc_token_count\": 500, \"add_transcription\": True, \"transcript_keyphrase_extract\": True, \"transcript_token_count\": 4096})\n",
    "# model_args[\"sandy-plasma-84\"] = new_args\n",
    "# new_args = common_args.copy()\n",
    "# new_args.update({\"model\": \"allenai/longformer-large-4096\", \"max_len\": 4096, \"freeze_lf_layers\": 23, \"add_title\": True, \"title_token_count\": 80, \"add_description\": True, \"desc_keyphrase_extract\": True, \"desc_token_count\": 400, \"add_transcription\": True, \"transcript_keyphrase_extract\": True, \"transcript_token_count\": 2200, \"add_other_comments\": True})\n",
    "# model_args[\"kind-sunset-85\"] = new_args\n",
    "# new_args = common_args.copy()\n",
    "new_args.update({\"model\": \"allenai/longformer-large-4096\", \"max_len\": 4096, \"freeze_lf_layers\": 23, \"add_title\": True, \"title_token_count\": 100, \"add_description\": True, \"desc_keyphrase_extract\": True, \"desc_token_count\": 500, \"add_transcription\": True, \"transcript_keyphrase_extract\": True, \"transcript_token_count\": 4096})\n",
    "model_args[\"fiery-brook-86\"] = new_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available devices: 0\n",
      "obtained dataloaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-large-4096 were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded models\n",
      "Training complete\n",
      "Best Model loaded\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 65\u001b[0m\n\u001b[1;32m     61\u001b[0m load_weights(model_name, lf_model, comment_model, args)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Model loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m test_loss, test_acc, test_pred, test_label \u001b[38;5;241m=\u001b[39m \u001b[43meval_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvision_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultitaskloss_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest: loss \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124maccu \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(test_loss, test_acc))\n\u001b[1;32m     67\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwork_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/npy_files/test_preds_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39marray(test_pred))\n",
      "Cell \u001b[0;32mIn[8], line 68\u001b[0m, in \u001b[0;36meval_one_epoch\u001b[0;34m(test_loader, epoch, phase, device, criterions, lf_model, vision_model, comment_model, multitaskloss_instance, args)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_video\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     66\u001b[0m     vis_emb \u001b[38;5;241m=\u001b[39m vision_model\u001b[38;5;241m.\u001b[39mget_embeddings(frames)\n\u001b[0;32m---> 68\u001b[0m output \u001b[38;5;241m=\u001b[39m comment_model(\u001b[43mlf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranscription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_comments\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m], vis_emb)\n\u001b[1;32m     70\u001b[0m loss_multilabel \u001b[38;5;241m=\u001b[39m criterions[\u001b[38;5;241m0\u001b[39m](output[\u001b[38;5;241m0\u001b[39m], label_multilabel)        \n\u001b[1;32m     71\u001b[0m loss_binary \u001b[38;5;241m=\u001b[39m criterions[\u001b[38;5;241m1\u001b[39m](output[\u001b[38;5;241m1\u001b[39m], label_binary)      \n",
      "Cell \u001b[0;32mIn[5], line 65\u001b[0m, in \u001b[0;36mLFEmbeddingModule.get_embeddings\u001b[0;34m(self, comments, titles, descriptions, transcripts, other_comments)\u001b[0m\n\u001b[1;32m     63\u001b[0m     indexed_cs\u001b[38;5;241m.\u001b[39mappend(enc_c)\n\u001b[1;32m     64\u001b[0m indexed_cs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(indexed_cs)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 65\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexed_cs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embedding\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:1750\u001b[0m, in \u001b[0;36mLongformerModel.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1742\u001b[0m extended_attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)[\n\u001b[1;32m   1743\u001b[0m     :, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, :\n\u001b[1;32m   1744\u001b[0m ]\n\u001b[1;32m   1746\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1747\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids, position_ids\u001b[38;5;241m=\u001b[39mposition_ids, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds\n\u001b[1;32m   1748\u001b[0m )\n\u001b[0;32m-> 1750\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1759\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1760\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:1326\u001b[0m, in \u001b[0;36mLongformerEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, padding_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1317\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m   1318\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1319\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1323\u001b[0m         is_index_global_attn,\n\u001b[1;32m   1324\u001b[0m     )\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1326\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1335\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;66;03m# bzs x seq_len x num_attn_heads x (num_global_attn + attention_window_len + 1) => bzs x num_attn_heads x seq_len x (num_global_attn + attention_window_len + 1)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:1249\u001b[0m, in \u001b[0;36mLongformerLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1241\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1248\u001b[0m ):\n\u001b[0;32m-> 1249\u001b[0m     self_attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1258\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m self_attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1259\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:1185\u001b[0m, in \u001b[0;36mLongformerAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   1176\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1177\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1184\u001b[0m ):\n\u001b[0;32m-> 1185\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_global_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1194\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m   1195\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attn_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:663\u001b[0m, in \u001b[0;36mLongformerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    654\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_attn_output_with_global_indices(\n\u001b[1;32m    655\u001b[0m         value_vectors\u001b[38;5;241m=\u001b[39mvalue_vectors,\n\u001b[1;32m    656\u001b[0m         attn_probs\u001b[38;5;241m=\u001b[39mattn_probs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    659\u001b[0m         is_local_index_global_attn_nonzero\u001b[38;5;241m=\u001b[39mis_local_index_global_attn_nonzero,\n\u001b[1;32m    660\u001b[0m     )\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;66;03m# compute local attn only\u001b[39;00m\n\u001b[0;32m--> 663\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sliding_chunks_matmul_attn_probs_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_sided_attn_window_size\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m (batch_size, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected size\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    668\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(seq_len, batch_size, embed_dim)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:926\u001b[0m, in \u001b[0;36mLongformerSelfAttention._sliding_chunks_matmul_attn_probs_value\u001b[0;34m(self, attn_probs, value, window_overlap)\u001b[0m\n\u001b[1;32m    918\u001b[0m chunked_value_stride \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    919\u001b[0m     chunked_value_stride[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    920\u001b[0m     window_overlap \u001b[38;5;241m*\u001b[39m chunked_value_stride[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    921\u001b[0m     chunked_value_stride[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    922\u001b[0m     chunked_value_stride[\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m    923\u001b[0m )\n\u001b[1;32m    924\u001b[0m chunked_value \u001b[38;5;241m=\u001b[39m padded_value\u001b[38;5;241m.\u001b[39mas_strided(size\u001b[38;5;241m=\u001b[39mchunked_value_size, stride\u001b[38;5;241m=\u001b[39mchunked_value_stride)\n\u001b[0;32m--> 926\u001b[0m chunked_attn_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pad_and_diagonalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunked_attn_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbcwd,bcdh->bcwh\u001b[39m\u001b[38;5;124m\"\u001b[39m, (chunked_attn_probs, chunked_value))\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context\u001b[38;5;241m.\u001b[39mview(batch_size, num_heads, seq_len, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch_cuda/lib/python3.9/site-packages/transformers/models/longformer/modeling_longformer.py:749\u001b[0m, in \u001b[0;36mLongformerSelfAttention._pad_and_diagonalize\u001b[0;34m(chunked_hidden_states)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;124;03mshift every row 1 step right, converting columns into diagonals.\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m               -0.0405, 0.1599, 0.0000 0.0000, 0.0000, 0.0000, 2.0514, -1.1600, 0.5372, 0.2629 ]\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    748\u001b[0m total_num_heads, num_chunks, window_overlap, hidden_dim \u001b[38;5;241m=\u001b[39m chunked_hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 749\u001b[0m chunked_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_overlap\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# total_num_heads x num_chunks x window_overlap x (hidden_dim+window_overlap+1). Padding value is not important because it'll be overwritten\u001b[39;00m\n\u001b[1;32m    752\u001b[0m chunked_hidden_states \u001b[38;5;241m=\u001b[39m chunked_hidden_states\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    753\u001b[0m     total_num_heads, num_chunks, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    754\u001b[0m )  \u001b[38;5;66;03m# total_num_heads x num_chunks x window_overlap*window_overlap+window_overlap\u001b[39;00m\n\u001b[1;32m    755\u001b[0m chunked_hidden_states \u001b[38;5;241m=\u001b[39m chunked_hidden_states[\n\u001b[1;32m    756\u001b[0m     :, :, :\u001b[38;5;241m-\u001b[39mwindow_overlap\n\u001b[1;32m    757\u001b[0m ]  \u001b[38;5;66;03m# total_num_heads x num_chunks x window_overlap*window_overlap\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for model_name, args in model_args.items():\n",
    "  device = \"mps\" if getattr(torch,'has_mps',False) else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "  print('number of available devices:', torch.cuda.device_count())\n",
    "\n",
    "  train_loader = get_data_loaders(args, 'train')\n",
    "  validation_loader = get_data_loaders(args, 'validation')\n",
    "  test_loader = get_data_loaders(args, 'test')\n",
    "  print('obtained dataloaders')\n",
    "\n",
    "  lf_model = LFEmbeddingModule(args, device)\n",
    "  vision_model = VisionModule(args, device)\n",
    "  comment_model = CommentModel(args).to(device)\n",
    "  multitaskloss_instance = MultiTaskLoss(n_tasks=2, reduction=\"sum\")\n",
    "\n",
    "  criterions = [nn.BCEWithLogitsLoss().to(device), nn.BCELoss().to(device)]\n",
    "\n",
    "  params = []\n",
    "  for model in [lf_model.lf_model, comment_model, multitaskloss_instance]:\n",
    "      params += list(model.parameters())\n",
    "\n",
    "  optimizer = optim.Adam(params, lr = args[\"lr\"])\n",
    "  scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "  print('loaded models')\n",
    "\n",
    "  if not os.path.exists(args[\"work_dir\"]):\n",
    "      os.mkdir(args[\"work_dir\"])\n",
    "\n",
    "  best_eval_loss = np.inf\n",
    "  train_acc = 0\n",
    "  eval_acc = 0\n",
    "  train_loss = 0\n",
    "  eval_loss = 0\n",
    "  # for epoch in range(args[\"max_epochs\"]):\n",
    "  #     train_loss, train_acc = train_one_epoch(train_loader, epoch, 'Train', device, criterions, optimizer, lf_model, vision_model, comment_model, multitaskloss_instance, args)\n",
    "  #     eval_loss, eval_acc, _, _ = eval_one_epoch(validation_loader, epoch, 'Eval', device, criterions, lf_model, vision_model, comment_model, multitaskloss_instance, args)\n",
    "  #     print('Epoch-{:<3d} Train: loss {:.4f}\\taccu {:.4f}\\tEval: loss {:.4f}\\taccu {:.4f}'\n",
    "  #             .format(epoch, train_loss, train_acc, eval_loss, eval_acc))\n",
    "  #     scheduler.step(eval_loss)\n",
    "  #     is_better = False\n",
    "  #     if eval_loss <= best_eval_loss:\n",
    "  #         best_eval_loss = eval_loss\n",
    "  #         is_better = True\n",
    "\n",
    "  #     save_checkpoint({ 'epoch': epoch,\n",
    "  #         'state_dict': lf_model.lf_model.state_dict(),\n",
    "  #         'best_loss': eval_loss,\n",
    "  #         'best_acc' : eval_acc,\n",
    "  #         'monitor': 'eval_acc',\n",
    "  #         'optimizer': optimizer.state_dict()\n",
    "  #     }, args, 'shrey', os.path.join(args['work_dir'], 'lf_model_' + '.pth.tar'), is_better)\n",
    "  #     save_checkpoint({ 'epoch': epoch ,\n",
    "  #         'state_dict': comment_model.state_dict(),\n",
    "  #         'best_loss': eval_loss,\n",
    "  #         'best_acc' : eval_acc,\n",
    "  #         'monitor': 'eval_acc',\n",
    "  #         'vpm_optimizer': optimizer.state_dict()\n",
    "  #     }, args, 'shrey', os.path.join(args['work_dir'], 'comment_model_' + '.pth.tar'), is_better)\n",
    "      \n",
    "  print(\"Training complete\")\n",
    "\n",
    "  load_weights(model_name, lf_model, comment_model, args)\n",
    "\n",
    "  print(\"Best Model loaded\")\n",
    "\n",
    "  test_loss, test_acc, test_pred, test_label = eval_one_epoch(test_loader, 0, 'Test', device, criterions, lf_model, vision_model, comment_model, multitaskloss_instance, args)\n",
    "  print('Test: loss {:.4f}\\taccu {:.4f}'.format(test_loss, test_acc))\n",
    "  np.save(f'{args[\"work_dir\"]}/npy_files/test_preds_{model_name}.npy', np.array(test_pred))\n",
    "  np.save(f'{args[\"work_dir\"]}/npy_files/test_labels_{model_name}.npy', np.array(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-13.3.1-arm64-arm-64bit\n",
      "PyTorch Version: 2.1.0.dev20230528\n",
      "\n",
      "Python 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:38:11) \n",
      "[Clang 14.0.6 ]\n",
      "Pandas 2.0.2\n",
      "Scikit-Learn 1.2.2\n",
      "GPU is NOT AVAILABLE\n",
      "MPS (Apple Metal) is AVAILABLE\n",
      "Target device is mps\n"
     ]
    }
   ],
   "source": [
    "# What version of Python do you have?\n",
    "import sys\n",
    "import platform\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = getattr(torch,'has_mps',False)\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(\"GPU is\", \"available\" if has_gpu else \"NOT AVAILABLE\")\n",
    "print(\"MPS (Apple Metal) is\", \"AVAILABLE\" if has_mps else \"NOT AVAILABLE\")\n",
    "print(f\"Target device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
