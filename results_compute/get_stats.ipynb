{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPY_FILE_PATH = \"../npy_files/9Jun_binary\"\n",
    "\n",
    "filenames = os.listdir(path=NPY_FILE_PATH)\n",
    "filenames = sorted(filenames, key=lambda x: x.split('_')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames\n",
    "filenames = list(set([filename.split('_')[2] for filename in filenames]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ancient-cosmos-225.npy'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 2, 8)\n"
     ]
    }
   ],
   "source": [
    "filename = filenames[0]\n",
    "labels_data = np.load(f'{NPY_FILE_PATH}/test_labels_{filename}', allow_pickle=True)\n",
    "preds_data = np.load(f'{NPY_FILE_PATH}/test_preds_{filename}', allow_pickle=True)\n",
    "\n",
    "\n",
    "print(labels_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.008255114778876305]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_data[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The metrics for ancient-cosmos-225 run are:\n",
      "     Precision: 0.8657407407407407\n",
      "     Precision for (Non Hate): 0.609375\n",
      "     Recall: 0.6515679442508711\n",
      "     Recall for (Non Hate): 0.8432432432432433\n",
      "     F1 score: 0.7435387673956262\n",
      "     F1 score for (Non Hate): 0.7074829931972789\n",
      "     Accuracy: 0.7266949152542372\n",
      "\n",
      "The metrics for giddy-cosmos-212 run are:\n",
      "     Precision: 0.8089887640449438\n",
      "     Precision for (Non Hate): 0.6536585365853659\n",
      "     Recall: 0.7526132404181185\n",
      "     Recall for (Non Hate): 0.7243243243243244\n",
      "     F1 score: 0.7797833935018051\n",
      "     F1 score for (Non Hate): 0.6871794871794872\n",
      "     Accuracy: 0.7415254237288136\n",
      "\n",
      "The metrics for fiery-tree-220 run are:\n",
      "     Precision: 0.8626609442060086\n",
      "     Precision for (Non Hate): 0.6401673640167364\n",
      "     Recall: 0.7003484320557491\n",
      "     Recall for (Non Hate): 0.827027027027027\n",
      "     F1 score: 0.773076923076923\n",
      "     F1 score for (Non Hate): 0.7216981132075472\n",
      "     Accuracy: 0.75\n",
      "\n",
      "The metrics for flowing-sun-220 run are:\n",
      "     Precision: 0.8540772532188842\n",
      "     Precision for (Non Hate): 0.6317991631799164\n",
      "     Recall: 0.6933797909407665\n",
      "     Recall for (Non Hate): 0.8162162162162162\n",
      "     F1 score: 0.7653846153846154\n",
      "     F1 score for (Non Hate): 0.7122641509433962\n",
      "     Accuracy: 0.7415254237288136\n",
      "\n",
      "The metrics for stellar-shadow-226 run are:\n",
      "     Precision: 0.9061032863849765\n",
      "     Precision for (Non Hate): 0.637065637065637\n",
      "     Recall: 0.6724738675958188\n",
      "     Recall for (Non Hate): 0.8918918918918919\n",
      "     F1 score: 0.772\n",
      "     F1 score for (Non Hate): 0.7432432432432432\n",
      "     Accuracy: 0.7584745762711864\n",
      "\n",
      "The metrics for divine-wave-212 run are:\n",
      "     Precision: 0.7322834645669292\n",
      "     Precision for (Non Hate): 0.536697247706422\n",
      "     Recall: 0.6480836236933798\n",
      "     Recall for (Non Hate): 0.6324324324324324\n",
      "     F1 score: 0.6876155268022182\n",
      "     F1 score for (Non Hate): 0.5806451612903226\n",
      "     Accuracy: 0.6419491525423728\n",
      "\n",
      "The metrics for olive-bush-212 run are:\n",
      "     Precision: 0.8666666666666667\n",
      "     Precision for (Non Hate): 0.6594827586206896\n",
      "     Recall: 0.7247386759581882\n",
      "     Recall for (Non Hate): 0.827027027027027\n",
      "     F1 score: 0.7893738140417457\n",
      "     F1 score for (Non Hate): 0.7338129496402879\n",
      "     Accuracy: 0.7648305084745762\n",
      "\n",
      "The metrics for legendary-galaxy-224 run are:\n",
      "     Precision: 0.800711743772242\n",
      "     Precision for (Non Hate): 0.675392670157068\n",
      "     Recall: 0.7839721254355401\n",
      "     Recall for (Non Hate): 0.6972972972972973\n",
      "     F1 score: 0.7922535211267606\n",
      "     F1 score for (Non Hate): 0.6861702127659575\n",
      "     Accuracy: 0.75\n",
      "\n",
      "The metrics for electric-planet-212 run are:\n",
      "     Precision: 0.8232931726907631\n",
      "     Precision for (Non Hate): 0.6322869955156951\n",
      "     Recall: 0.7142857142857143\n",
      "     Recall for (Non Hate): 0.7621621621621621\n",
      "     F1 score: 0.7649253731343284\n",
      "     F1 score for (Non Hate): 0.6911764705882353\n",
      "     Accuracy: 0.7330508474576272\n",
      "\n",
      "The metrics for royal-morning-217 run are:\n",
      "     Precision: 0.8445378151260504\n",
      "     Precision for (Non Hate): 0.6324786324786325\n",
      "     Recall: 0.7003484320557491\n",
      "     Recall for (Non Hate): 0.8\n",
      "     F1 score: 0.7657142857142857\n",
      "     F1 score for (Non Hate): 0.7064439140811456\n",
      "     Accuracy: 0.739406779661017\n",
      "\n",
      "The metrics for sandy-fire-217 run are:\n",
      "     Precision: 0.7674418604651163\n",
      "     Precision for (Non Hate): 0.5841121495327103\n",
      "     Recall: 0.6898954703832753\n",
      "     Recall for (Non Hate): 0.6756756756756757\n",
      "     F1 score: 0.7266055045871559\n",
      "     F1 score for (Non Hate): 0.6265664160401003\n",
      "     Accuracy: 0.684322033898305\n",
      "\n",
      "The metrics for sparkling-resonance-217 run are:\n",
      "     Precision: 0.0\n",
      "     Precision for (Non Hate): 0.3919491525423729\n",
      "     Recall: 0.0\n",
      "     Recall for (Non Hate): 1.0\n",
      "     F1 score: 0.0\n",
      "     F1 score for (Non Hate): 0.5631659056316591\n",
      "     Accuracy: 0.3919491525423729\n",
      "\n",
      "The metrics for apricot-frog-220 run are:\n",
      "     Precision: 0.8854625550660793\n",
      "     Precision for (Non Hate): 0.6489795918367347\n",
      "     Recall: 0.7003484320557491\n",
      "     Recall for (Non Hate): 0.8594594594594595\n",
      "     F1 score: 0.7821011673151751\n",
      "     F1 score for (Non Hate): 0.7395348837209302\n",
      "     Accuracy: 0.7627118644067796\n",
      "\n",
      "The metrics for floral-surf-227 run are:\n",
      "     Precision: 0.867579908675799\n",
      "     Precision for (Non Hate): 0.616600790513834\n",
      "     Recall: 0.662020905923345\n",
      "     Recall for (Non Hate): 0.8432432432432433\n",
      "     F1 score: 0.75098814229249\n",
      "     F1 score for (Non Hate): 0.7123287671232876\n",
      "     Accuracy: 0.7330508474576272\n",
      "\n",
      "The metrics for frosty-cherry-213 run are:\n",
      "     Precision: 0.0\n",
      "     Precision for (Non Hate): 0.3919491525423729\n",
      "     Recall: 0.0\n",
      "     Recall for (Non Hate): 1.0\n",
      "     F1 score: 0.0\n",
      "     F1 score for (Non Hate): 0.5631659056316591\n",
      "     Accuracy: 0.3919491525423729\n",
      "\n",
      "The metrics for whole-elevator-220 run are:\n",
      "     Precision: 0.8268398268398268\n",
      "     Precision for (Non Hate): 0.6016597510373444\n",
      "     Recall: 0.6655052264808362\n",
      "     Recall for (Non Hate): 0.7837837837837838\n",
      "     F1 score: 0.7374517374517374\n",
      "     F1 score for (Non Hate): 0.6807511737089201\n",
      "     Accuracy: 0.711864406779661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for filename in filenames:\n",
    "    run_name = filename.split('.')[0]\n",
    "    labels_data = np.load(f'{NPY_FILE_PATH}/test_labels_{filename}', allow_pickle=True)\n",
    "    preds_data = np.load(f'{NPY_FILE_PATH}/test_preds_{filename}', allow_pickle=True)\n",
    "    # else:\n",
    "    #     labels_data = np.load(f'{NPY_FILE_PATH}/{filenames[file_count+1]}', allow_pickle=True)\n",
    "    #     preds_data = np.load(f'{NPY_FILE_PATH}/{filenames[file_count]}', allow_pickle=True)\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for i in range(len(labels_data)):\n",
    "        for j in range(len(labels_data[i][0])):\n",
    "            labels.append(labels_data[i][0][j][0])\n",
    "            preds.append(preds_data[i][0][j][0])\n",
    "    labels = np.array(labels)\n",
    "    preds = np.array(preds)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    \n",
    "    # preds = 1/(1 + np.exp(-preds))\n",
    "    print(f\"\\nThe metrics for {run_name} run are:\")\n",
    "    print(\"     Precision:\", precision_score(labels, preds, pos_label=1))\n",
    "    print(\"     Precision for (Non Hate):\", precision_score(labels, preds, pos_label=0))\n",
    "    print(\"     Recall:\", recall_score(labels, preds, pos_label=1))\n",
    "    print(\"     Recall for (Non Hate):\", recall_score(labels, preds, pos_label=0))\n",
    "    print(\"     F1 score:\", f1_score(labels, preds, pos_label=1))\n",
    "    print(\"     F1 score for (Non Hate):\", f1_score(labels, preds, pos_label=0))\n",
    "    print(\"     Accuracy:\", accuracy_score(labels, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05653011053800583\n",
      "0.9987609386444092\n",
      "0.949430525302887\n",
      "0.9984368681907654\n",
      "0.9973963499069214\n",
      "0.6387729644775391\n",
      "0.19644924998283386\n",
      "0.23118489980697632\n",
      "0.9954143762588501\n",
      "0.9946818947792053\n",
      "0.9854972958564758\n",
      "0.487801194190979\n",
      "0.9666085839271545\n",
      "0.03705838695168495\n",
      "0.038530219346284866\n",
      "[0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "for batch in preds_data:\n",
    "    for el in batch:\n",
    "        print(el[-1])\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
